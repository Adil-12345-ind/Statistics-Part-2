{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics Part 2"
      ],
      "metadata": {
        "id": "MPDcwGlswj0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is hypothesis testing in statistics?\n",
        " - Hypothesis testing in statistics is a method used to assess a claim about a population parameter using sample data. It involves formulating a null hypothesis (a statement of no effect or no difference) and an alternative hypothesis (a statement of what the researcher believes to be true). By analyzing sample data, we determine whether there's enough evidence to reject the null hypothesis in favor of the alternative.\n",
        "Statistical Hypothesis Testing step by step procedure ...\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "Formulating Hypotheses:\n",
        "Null Hypothesis (H0):\n",
        "This is the initial assumption, often stating no effect, no difference, or no relationship. For example, \"The average weight of a dumbbell is 90lbs\".\n",
        "Alternative Hypothesis (Ha):\n",
        "This is the researcher's claim, contradicting the null hypothesis. For example, \"The average weight of a dumbbell is greater than 90lbs\".\n",
        "\n",
        "Steps Involved:\n",
        "State the Hypotheses: Define the null and alternative hypotheses based on the research question.\n",
        "Collect Data: Gather relevant sample data from the population of interest.\n",
        "Analyze Data: Calculate a test statistic (e.g., t-statistic, z-statistic) to summarize the evidence from the sample data.\n",
        "Determine p-value: The p-value represents the probability of observing results as extreme as, or more extreme than, the sample results, assuming the null hypothesis is true.\n",
        "Make a Decision: Compare the p-value to a significance level (alpha), typically 0.05. If the p-value is less than or equal to alpha, reject the null hypothesis; otherwise, fail to reject it.\n",
        "\n",
        "Examples:\n",
        "Quality Control:\n",
        "A manufacturer might test the hypothesis that the average weight of their product is within the specified range.\n",
        "Medical Research:\n",
        "A researcher might test the hypothesis that a new drug is effective in treating a disease.\n",
        "Social Sciences:\n",
        "A researcher might test the hypothesis that a new teaching method improves student performance.\n",
        "\n",
        "Key Concepts:\n",
        "Significance Level (alpha):\n",
        "The threshold for rejecting the null hypothesis, usually set at 0.05.\n",
        "p-value:\n",
        "The probability of observing the sample results (or more extreme) if the null hypothesis is true.\n",
        "Test Statistic:\n",
        "A value calculated from sample data to assess the evidence against the null hypothesis.\n",
        "Conclusion:\n",
        "Rejecting or failing to reject the null hypothesis based on the p-value and significance level.\n"
      ],
      "metadata": {
        "id": "zU_NS0OXwwy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the null hypothesis, and how does it differ from the alternative hypothesis?\n",
        " - The null hypothesis is a statement of \"no effect\" or \"no difference,\" while the alternative hypothesis is a statement that contradicts the null hypothesis, suggesting there is a significant effect or difference. In essence, researchers aim to disprove or reject the null hypothesis in favor of the alternative hypothesis.\n",
        "Here's a more detailed breakdown:\n",
        "Null Hypothesis (H0):\n",
        "Definition: The null hypothesis is a statement that assumes there is no relationship, no effect, or no difference between variables in a population.\n",
        "Purpose: It serves as the default position that researchers try to disprove.\n",
        "Example: \"There is no difference in test scores between students who study with flashcards and those who don't.\"\n",
        "Alternative Hypothesis (Ha or H1):\n",
        "Definition: The alternative hypothesis is a statement that contradicts the null hypothesis and suggests there is a relationship, effect, or difference.\n",
        "Purpose: It's the claim that researchers are trying to find evidence for.\n",
        "Example: \"Students who study with flashcards will have significantly higher test scores than those who don't.\"\n",
        "Key Differences Summarized:\n",
        "Feature\n",
        "Null Hypothesis (H0)\n",
        "Alternative Hypothesis (Ha or H1)\n",
        "Statement\n",
        "No effect, no difference\n",
        "Effect, difference\n",
        "Goal of Research\n",
        "To be rejected\n",
        "To be accepted\n",
        "Assumed True?\n",
        "Yes, initially\n",
        "No, it contradicts the null hypothesis\n",
        "Example\n",
        "No difference in test scores between two groups\n",
        "There is a difference in test scores between two groups\n"
      ],
      "metadata": {
        "id": "2R9b2KRMxsil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the significance level in hypothesis testing, and why is it important?\n",
        "- In hypothesis testing, the significance level (alpha, α) is the probability of rejecting the null hypothesis when it is actually true. It represents the researcher's predetermined threshold for how much risk they're willing to take in incorrectly rejecting the null hypothesis (a false positive). A lower significance level indicates a stricter standard for rejecting the null hypothesis, requiring stronger evidence to do so.\n",
        "Here's a more detailed explanation:\n",
        "Significance Level (α):\n",
        "Definition:\n",
        "The significance level (α) is a probability value that is set before the hypothesis test begins. It represents the maximum probability of making a Type I error, which is rejecting a true null hypothesis.\n",
        "Common Values:\n",
        "Commonly used significance levels are 0.05 (5%), 0.01 (1%), and 0.10 (10%).\n",
        "Purpose:\n",
        "It acts as a threshold. If the p-value (the probability of observing the data, or more extreme data, if the null hypothesis is true) is less than or equal to α, the null hypothesis is rejected. Otherwise, it is not rejected.\n",
        "Why is Significance Level Important?\n",
        "\n",
        "Controlling Type I Error:\n",
        "The significance level directly controls the risk of making a Type I error (false positive). By setting α, researchers determine how likely they are to mistakenly conclude there's a significant effect when there isn't.\n",
        "\n",
        "Balancing Risk:\n",
        "Choosing the significance level involves a trade-off. A lower α reduces the risk of Type I errors but increases the risk of failing to reject a false null hypothesis (Type II error). A higher α increases the risk of Type I errors but decreases the risk of Type II errors.\n",
        "\n",
        "Contextual Decision:\n",
        "The appropriate significance level depends on the context of the research. In fields like medical research, where the consequences of a false positive can be severe, a lower α (e.g., 0.01) might be chosen. In exploratory studies, a higher α (e.g., 0.10) might be more appropriate to avoid missing potential effects.\n",
        "\n",
        "Setting the Standard:\n",
        "The significance level establishes the standard for statistical significance. It helps researchers determine whether their findings are likely due to a real effect or just random chance."
      ],
      "metadata": {
        "id": "StTeub9VyAY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does a P-value represent in hypothesis testing?\n",
        " - In hypothesis testing, a p-value represents the probability of observing results as extreme as, or more extreme than, the results obtained in a study, assuming the null hypothesis is true. Essentially, it quantifies the evidence against the null hypothesis; a smaller p-value indicates stronger evidence against the null hypothesis.\n",
        "Here's a more detailed explanation:\n",
        "Null Hypothesis:\n",
        "The null hypothesis is a statement about the population that the researcher aims to disprove. For example, it might state that there is no difference between two groups, or that a treatment has no effect.\n",
        "Observed Results:\n",
        "These are the findings from the actual study or experiment.\n",
        "P-value Calculation:\n",
        "The p-value is calculated based on the observed results and the assumption that the null hypothesis is true. It answers the question: \"If the null hypothesis were true, what is the probability of getting results as extreme as, or more extreme than, the results we actually observed?\"\n",
        "Interpreting the P-value:\n",
        "A small p-value (typically below a predetermined significance level, often 0.05) suggests that the observed results are unlikely to have occurred by chance alone if the null hypothesis were true. This leads to rejecting the null hypothesis.\n",
        "A large p-value suggests that the observed results are likely to have occurred by chance, even if the null hypothesis is true. This leads to failing to reject the null hypothesis.\n",
        "Example:\n",
        "If a study finds a p-value of 0.02 for the difference between two drug treatments, it means there's a 2% chance of observing such a large difference in treatment outcomes if the drugs were actually equally effective. This 2% chance is considered small enough to reject the null hypothesis (that the drugs are equally effective) and conclude there's a statistically significant difference."
      ],
      "metadata": {
        "id": "YQQ3ZAY_yRH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do you interpret the P-value in hypothesis testing?\n",
        " - In hypothesis testing, the p-value represents the probability of observing results as extreme as, or more extreme than, the results obtained in a study, assuming the null hypothesis is true. A smaller p-value indicates stronger evidence against the null hypothesis, suggesting that the observed results are unlikely to have occurred by random chance alone.\n",
        "Here's a more detailed explanation:\n",
        "Null Hypothesis:\n",
        "The null hypothesis is a statement that there is no effect, no difference, or no relationship between variables in a population.\n",
        "P-value Interpretation:\n",
        "The p-value helps determine whether to reject or fail to reject the null hypothesis.\n",
        "Significance Level (α):\n",
        "Researchers typically set a significance level (often 0.05) before conducting the study. This value represents the threshold for statistical significance.\n",
        "Decision:\n",
        "If the p-value is less than or equal to the significance level (p ≤ α), the null hypothesis is rejected. This suggests that the observed results are statistically significant and unlikely due to chance.\n",
        "If the p-value is greater than the significance level (p > α), the null hypothesis is not rejected. This indicates that the observed results are not statistically significant and could reasonably have occurred by chance."
      ],
      "metadata": {
        "id": "Si8vIU2uyv_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What are Type 1 and Type 2 errors in hypothesis testing?\n",
        " - Type I and Type II Errors - GeeksforGeeksIn hypothesis testing, Type I and Type II errors are the two types of mistakes that can occur when making decisions about a null hypothesis. A Type I error (false positive) occurs when the null hypothesis is rejected, even though it is actually true. Conversely, a Type II error (false negative) occurs when the null hypothesis is not rejected, even though it is actually false.\n",
        "Type I Error (False Positive):\n",
        "Definition: Rejecting a true null hypothesis.\n",
        "Example: Concluding that a new drug is effective when it actually isn't.\n",
        "Probability: Represented by the significance level (alpha), typically set at 0.05 (5%).\n",
        "Type II Error (False Negative):\n",
        "Definition: Failing to reject a false null hypothesis.\n",
        "Example: Failing to detect that a new drug is effective when it actually is.\n",
        "Probability: Represented by beta (β), related to statistical power (1-β).\n",
        "Relationship between Type I and Type II errors:\n",
        "These errors are inversely related; reducing one may increase the risk of the other.\n",
        "For example, lowering the significance level (alpha) to decrease Type I errors can increase the probability of a Type II error.\n",
        "Researchers often aim to balance the risk of both types of errors based on the specific context of the study"
      ],
      "metadata": {
        "id": "Rn6SAxfzzHOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is the difference between a one-tailed and a two-tailed test in hypothesis testing?\n",
        " - In hypothesis testing, a one-tailed test examines if a parameter is greater than or less than a specific value, while a two-tailed test examines if it is simply different (either greater or less than). Essentially, a one-tailed test focuses on a single direction, whereas a two-tailed test considers both directions.\n",
        "Key Differences:\n",
        "Directionality:\n",
        "One-tailed tests are directional, specifying whether a parameter is greater or less than a value. Two-tailed tests are non-directional, simply testing for a difference.\n",
        "Alternative Hypothesis:\n",
        "In a one-tailed test, the alternative hypothesis specifies a direction (e.g., the mean is greater than a certain value). In a two-tailed test, the alternative hypothesis only states that the parameter is different from the value in the null hypothesis, without specifying direction.\n",
        "Region of Rejection:\n",
        "One-tailed tests have the region of rejection on one side of the distribution (either the left or right tail). Two-tailed tests have the rejection region split between both tails of the distribution.\n",
        "Power:\n",
        "One-tailed tests are more powerful (more likely to detect a difference) if the direction of the effect is correctly predicted, as the entire significance level is focused on one tail. Two-tailed tests are more conservative, requiring a larger effect size to be statistically significant, as the significance level is split across both tails.\n",
        "Example:\n",
        "One-tailed:\n",
        "Testing if a new drug increases patient recovery time (looking only for an increase, not a decrease).\n",
        "Two-tailed:\n",
        "Testing if a new advertising campaign changes consumer behavior (looking for any change, either an increase or decrease in purchases)."
      ],
      "metadata": {
        "id": "eB530ao1zc1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Z-test, and when is it used in hypothesis testing?\n",
        " - A z-test is a statistical hypothesis test used to determine if there's a significant difference between a sample mean and a population mean, or between the means of two samples, when the population variances are known or the sample size is large. It's based on the normal distribution and is often used when dealing with large sample sizes (n ≥ 30).\n",
        "Here's a breakdown:\n",
        "When to use a z-test:\n",
        "Large sample sizes:\n",
        "When you have a large sample (typically n ≥ 30), the z-test is a suitable choice, especially if the population variance is known or can be reasonably estimated.\n",
        "Known population variance:\n",
        "If the population standard deviation is known, the z-test is appropriate.\n",
        "Comparing sample mean to a population mean:\n",
        "To determine if a sample mean significantly differs from a hypothesized population mean.\n",
        "Comparing means of two independent samples:\n",
        "To assess if two sample means are significantly different from each other, assuming known population variances.\n",
        "Key points about z-tests:\n",
        "Assumes normality:\n",
        "Z-tests generally assume that the data follows a normal distribution.\n",
        "One-sample vs. two-sample:\n",
        "Z-tests can be used for both one-sample (comparing a sample to a population) and two-sample (comparing two samples) scenarios.\n",
        "One-tailed vs. two-tailed:\n",
        "The z-test can be one-tailed (testing if a mean is greater or less than a value) or two-tailed (testing if a mean is different from a value).\n",
        "Relationship with t-tests:\n",
        "When the population variance is unknown and the sample size is small (n < 30), a t-test is typically used instead of a z-test."
      ],
      "metadata": {
        "id": "4lKI-6IrzxkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How do you calculate the Z-score, and what does it represent in hypothesis testing?\n",
        " - A z-score measures how many standard deviations a data point is away from the mean of a dataset. In hypothesis testing, it represents the test statistic, indicating how far the sample mean deviates from the hypothesized population mean, in terms of standard errors. This allows us to assess the likelihood of observing such a deviation if the null hypothesis were true.\n",
        "Calculating the Z-score:\n",
        "The formula for calculating a z-score is:\n",
        "z = (x - μ) / σ\n",
        "Where:\n",
        "z: is the z-score.\n",
        "x: is the data point (or sample mean in hypothesis testing).\n",
        "μ: is the population mean.\n",
        "σ: is the population standard deviation.\n",
        "If the population standard deviation is unknown, it can be estimated using the sample standard deviation (s), and the formula becomes: z = (x - μ) / (s / √n), where n is the sample size.\n",
        "Z-score in Hypothesis Testing:\n",
        "In hypothesis testing, the z-score helps determine if the observed difference between a sample and a population (or between two samples) is statistically significant. Here's how:\n",
        "1. Calculate the z-score:\n",
        "You calculate the z-score using the sample mean, population mean, and standard deviation (or sample standard deviation and sample size). This z-score represents how many standard errors the sample mean is away from the hypothesized population mean.\n",
        "2. Determine the critical value:\n",
        "Based on the chosen significance level (alpha) and whether it's a one-tailed or two-tailed test, you find the critical z-value(s) from a z-table.\n",
        "3. Compare:\n",
        "If the calculated z-score falls outside the range of the critical values (i.e., it's more extreme), you reject the null hypothesis. This indicates that the observed difference is unlikely to have occurred by chance alone, and there is evidence to support the alternative hypothesis. If the calculated z-score falls within the range of the critical values, you fail to reject the null hypothesis.\n",
        "Example:\n",
        "Let's say you're testing if the average height of students in your class is different from the national average height. You collect data on your class (sample) and calculate the sample mean and standard deviation. You also know the national average height (population mean) and the national standard deviation.\n",
        "You calculate the z-score using the formula above, plugging in the sample mean, population mean, and standard deviation.\n",
        "You set your significance level (alpha) to 0.05 and perform a two-tailed test. This means you'll reject the null hypothesis if the z-score is less than -1.96 or greater than 1.96.\n",
        "If your calculated z-score is 2.5, it falls outside the range of the critical values, indicating a statistically significant difference between the class average and the national average. You would reject the null hypothesis.\n"
      ],
      "metadata": {
        "id": "vW6_oxhjz949"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the T-distribution, and when should it be used instead of the normal distribution?\n",
        " - The t-distribution, also known as Student's t-distribution, is a probability distribution used in statistics, particularly when dealing with small sample sizes or when the population standard deviation is unknown. It is a suitable alternative to the normal distribution in these scenarios because it accounts for the uncertainty introduced by estimating the population standard deviation from a sample. The t-distribution is bell-shaped and symmetric like the normal distribution, but it has heavier tails, meaning it has a higher probability of extreme values.\n",
        "When to Use the T-Distribution:\n",
        "Small Sample Sizes:\n",
        "When the sample size (n) is small (typically less than 30), and the population standard deviation is unknown, the t-distribution is preferred over the normal distribution.\n",
        "Unknown Population Standard Deviation:\n",
        "If the population standard deviation is not known, and you are using the sample standard deviation to estimate it, the t-distribution is more appropriate, especially for smaller samples.\n",
        "Confidence Intervals and Hypothesis Testing:\n",
        "The t-distribution is commonly used in constructing confidence intervals for population means and in conducting hypothesis tests (t-tests) when dealing with small samples or unknown population standard deviations.\n",
        "Why Use the T-Distribution Instead of the Normal Distribution in These Cases?\n",
        "Normal Distribution Assumption:\n",
        "The normal distribution assumes you know the population standard deviation, which is often unrealistic in practical scenarios, especially with small samples.\n",
        "Overestimation of Precision:\n",
        "When using the normal distribution with estimated standard deviations from small samples, you might overestimate the precision of your results, leading to potentially misleading conclusions.\n",
        "T-distribution's Heavier Tails:\n",
        "The t-distribution's heavier tails account for the increased uncertainty associated with estimating the population standard deviation from a small sample, providing a more accurate representation of the data's variability.\n",
        "More Conservative Results:\n",
        "The t-distribution generally leads to wider confidence intervals and more conservative hypothesis test results compared to using the normal distribution with estimated standard deviations, which can be more realistic when dealing with uncertainty."
      ],
      "metadata": {
        "id": "DunGaS2x0I6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is the difference between a Z-test and a T-test?\n",
        " - The primary difference between a Z-test and a T-test lies in the information available about the population and the sample size. Z-tests are used when the population standard deviation is known and the sample size is large (generally > 30). T-tests are used when the population standard deviation is unknown and/or the sample size is small (generally < 30).\n",
        "Here's a more detailed breakdown:\n",
        "Z-Test:\n",
        "Population Standard Deviation: Assumes the population standard deviation is known.\n",
        "Sample Size: Requires a large sample size.\n",
        "Distribution: Assumes the population is normally distributed.\n",
        "Test Statistic: Uses the z-statistic.\n",
        "When to Use: Suitable for situations where you have a large sample and know the population's variability (standard deviation)."
      ],
      "metadata": {
        "id": "d4S7r0iK0oRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the T-test, and how is it used in hypothesis testing?\n",
        " - A t-test is a statistical hypothesis test used to determine if there's a significant difference between the means of two groups. It's commonly used in hypothesis testing to assess whether observed differences between sample means are likely due to a real effect or simply random chance.\n",
        "t-test : Definition and Example - Shiksha Online\n",
        "Here's how it's used in hypothesis testing:\n",
        "1. Define Hypotheses:\n",
        "A t-test begins with formulating null and alternative hypotheses. The null hypothesis (H0) typically assumes no significant difference between the means of the groups being compared. The alternative hypothesis (Ha) suggests there is a significant difference.\n",
        "2. Choose a t-test type:\n",
        "There are different types of t-tests, including independent samples t-tests (for comparing two independent groups) and paired samples t-tests (for comparing related groups, like before-and-after measurements on the same individuals).\n",
        "3. Calculate the t-statistic:\n",
        "The t-test involves calculating a t-statistic based on the sample data. This statistic quantifies the difference between the means relative to the variability within the groups.\n",
        "4. Determine the p-value:\n",
        "The calculated t-statistic is then used to find a p-value. The p-value represents the probability of observing a difference as extreme as, or more extreme than, the one calculated, assuming the null hypothesis is true.\n",
        "5. Make a decision:\n",
        "If the p-value is below a predetermined significance level (alpha, typically 0.05), the null hypothesis is rejected, suggesting a statistically significant difference between the means. If the p-value is greater than alpha, the null hypothesis is not rejected.\n",
        "6. Interpret results:\n",
        "The results of a t-test help determine if a treatment, intervention, or any other factor has a meaningful impact on the groups being compared."
      ],
      "metadata": {
        "id": "lRdVxDX_0-Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the relationship between Z-test and T-test in hypothesis testing?\n",
        " - Both z-tests and t-tests are used in hypothesis testing to compare means, but they differ in their assumptions and when they are appropriate to use. Z-tests are used when the population standard deviation is known or the sample size is large (generally greater than 30), while t-tests are used when the population standard deviation is unknown and the sample size is small (generally less than 30).\n",
        "Key Differences:\n",
        "Population Standard Deviation:\n",
        "Z-tests assume the population standard deviation is known, while t-tests assume it's unknown.\n",
        "Sample Size:\n",
        "Z-tests are typically used for larger sample sizes, while t-tests are more appropriate for smaller sample sizes.\n",
        "Test Statistic:\n",
        "Z-tests use the z-statistic, which follows a standard normal distribution, while t-tests use the t-statistic, which follows a t-distribution.\n",
        "Degrees of Freedom:\n",
        "T-tests involve calculating degrees of freedom (n-1), while z-tests do not.\n",
        "Critical Values:\n",
        "Z-test critical values are derived from the standard normal distribution, while t-test critical values are derived from the t-distribution, which varies based on degrees of freedom.\n",
        "Relationship:\n",
        "Despite their differences, both tests share the same underlying goal of hypothesis testing: to determine if there is a statistically significant difference between a sample mean and a population mean or between two sample means.\n",
        "Parametric Tests:\n",
        "Both z-tests and t-tests are parametric tests, meaning they assume the data follows a specific distribution (usually normal).\n",
        "Inference:\n",
        "Both tests are used to make inferences about a population based on sample data.\n",
        "Decision Making:\n",
        "The choice between a z-test and a t-test depends on the specific characteristics of the data and the research question being asked.\n",
        "Large Sample Size:\n",
        "When the sample size is large enough (generally n > 30), the t-distribution approaches the z-distribution, and the results of the two tests will be very similar."
      ],
      "metadata": {
        "id": "pK8SdYBA1LWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is a confidence interval, and how is it used to interpret statistical results?\n",
        " - A confidence interval (CI) is a range of values that is likely to contain the true value of an unknown population parameter, like a mean or proportion. It's used to quantify the uncertainty associated with an estimate derived from a sample of data. The confidence level associated with the interval (e.g., 95%, 99%) indicates the probability that the interval will contain the true population parameter if the sampling process were repeated many times.\n",
        "How it's used to interpret statistical results:\n",
        "\n",
        "Understanding the Precision of an Estimate:\n",
        "A confidence interval provides a range of plausible values for the population parameter, rather than just a single point estimate. This range gives an indication of the precision of the estimate. A narrower interval suggests a more precise estimate, while a wider interval indicates more uncertainty.\n",
        "\n",
        "Assessing the Significance of Results:\n",
        "If a confidence interval for the difference between two groups (e.g., the effect of a treatment) does not include zero, it suggests that the observed difference is unlikely to be due to random chance and is statistically significant.\n",
        "\n",
        "Making Inferences about the Population:\n",
        "By providing a range of plausible values, confidence intervals allow researchers to make inferences about the population based on sample data. For example, a 95% confidence interval for the average height of students in a class can be used to estimate the average height of all students in the school.\n",
        "\n",
        "\n",
        "Comparing Results Across Studies:\n",
        "Confidence intervals can be used to compare results from different studies. If the confidence intervals for the same parameter in two studies overlap, it suggests that the results are consistent. If they do not overlap, it suggests a statistically significant difference between the studies.\n",
        "\n",
        "Evaluating the Reliability of Predictions:\n",
        "In areas like finance, confidence intervals can be used to assess the reliability of predictions. For example, a confidence interval for future stock prices can help investors understand the potential range of outcomes.\n",
        "Important Considerations:\n",
        "A confidence interval does not tell you the probability that the true population parameter lies within the calculated interval. It indicates the probability that the process used to create the interval will produce an interval that contains the true parameter if repeated many times.\n",
        "Confidence intervals can be influenced by sample size and variability in the data. A larger sample size and lower variability will generally lead to a narrower confidence interval.\n"
      ],
      "metadata": {
        "id": "HDlBXpVw1Wkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is the margin of error, and how does it affect the confidence interval?\n",
        " - The margin of error quantifies the uncertainty in a statistical estimate, representing the range around a sample statistic (like a sample mean) where the true population parameter is likely to fall. A larger margin of error means less precision in the estimate, while a smaller margin of error indicates more confidence in the estimate. The margin of error directly impacts the width of a confidence interval; a larger margin of error results in a wider confidence interval, and a smaller margin of error results in a narrower confidence interval.\n",
        "Here's a more detailed explanation:\n",
        "Margin of Error:\n",
        "Definition:\n",
        "The margin of error is a statistic that expresses the amount of random sampling error in a survey result. It's essentially a measure of how much the sample results might differ from the true population value.\n",
        "Impact:\n",
        "A larger margin of error means there's a greater chance that the true population value falls outside the range suggested by the sample data. Conversely, a smaller margin of error indicates a higher degree of confidence that the sample accurately reflects the population.\n",
        "Confidence Interval:\n",
        "Definition:\n",
        "A confidence interval is a range of values within which the true population parameter is likely to fall, given a certain level of confidence (e.g., 95%).\n",
        "Impact:\n",
        "The margin of error is used to calculate the confidence interval. It's added and subtracted from the sample statistic (e.g., sample mean) to create the upper and lower bounds of the interval.\n",
        "Relationship:\n",
        "Wider Interval, Larger Margin of Error:\n",
        "If you want to be more certain that the true population value falls within your interval (e.g., a 99% confidence level instead of 95%), you'll need a wider interval. This wider interval is achieved by increasing the margin of error.\n",
        "Narrower Interval, Smaller Margin of Error:\n",
        "Conversely, if you're willing to accept a lower level of confidence (e.g., a 90% confidence level), you can use a narrower interval. This is achieved by decreasing the margin of error.\n",
        "Example:\n",
        "If a survey has a 95% confidence interval of 49% to 55% with a 3% margin of error, it means there's a 95% chance the true population value lies between 49% and 55%. If you wanted a 99% confidence interval, the margin of error would likely be larger, resulting in a wider interval, perhaps from 47% to 57%, according to Nielsen Norman Group.\n",
        "Factors Affecting Margin of Error:\n",
        "Sample Size:\n",
        "Larger sample sizes generally lead to smaller margins of error because you have more data to estimate the population parameter.\n",
        "Confidence Level:\n",
        "Higher confidence levels require larger margins of error, as you're trying to be more certain that the true value falls within the interval.\n",
        "Population Variability:\n",
        "The more spread there is in the population data, the larger the margin of error will be for a given sample size."
      ],
      "metadata": {
        "id": "1BeDSpwR1oLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How is Bayes' Theorem used in statistics, and what is its significance?\n",
        " - Bayes' Theorem is a fundamental concept in probability and statistics used to update the probability of an event based on new evidence. It allows us to combine prior knowledge with observed data to refine our understanding of the likelihood of that event. In essence, it provides a way to calculate conditional probabilities, specifically, the probability of a cause given its effect, or the probability of a hypothesis given observed evidence.\n",
        "Here's how Bayes' Theorem is used in statistics:\n",
        "\n",
        "Updating Probabilities:\n",
        "Bayes' Theorem provides a formula to calculate the probability of an event (A) given that another event (B) has already occurred. This is represented as P(A|B) and is called the posterior probability.\n",
        "It uses the prior probability of event A (P(A)), the probability of event B given A (P(B|A)), and the probability of event B (P(B)) to calculate the posterior probability.\n",
        "The formula is: P(A|B) = [P(B|A) * P(A)] / P(B).\n",
        "\n",
        "Applications in Statistical Inference:\n",
        "Bayesian Inference:\n",
        "Bayes' Theorem is the foundation of Bayesian statistics, a framework for statistical inference where probabilities represent degrees of belief and are updated as new evidence becomes available.\n",
        "Model Evaluation:\n",
        "In machine learning, it helps evaluate the probability of a model's parameters given observed data, allowing for model selection and refinement.\n",
        "Hypothesis Testing:\n",
        "It can be used to calculate the probability of a hypothesis being true given the observed data, aiding in decision-making.\n",
        "Risk Assessment:\n",
        "In fields like finance and healthcare, it helps assess risks by updating probabilities based on new information.\n",
        "\n",
        "Significance:\n",
        "Incorporating Prior Knowledge:\n",
        "Bayes' Theorem allows us to incorporate prior knowledge or beliefs about an event into the analysis, making it a powerful tool for updating our understanding as new information emerges.\n",
        "Dynamic Updating:\n",
        "It enables us to dynamically update probabilities as new data becomes available, leading to more accurate predictions and decisions.\n",
        "Handling Uncertainty:\n",
        "By quantifying uncertainty through probabilities, Bayes' Theorem provides a framework for making decisions in the face of incomplete or uncertain information.\n",
        "Versatile Applications:\n",
        "It has diverse applications in various fields, including machine learning, medical diagnosis, weather forecasting, and more."
      ],
      "metadata": {
        "id": "yG08yHIL12Vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is the Chi-square distribution, and when is it used?\n",
        " - The chi-square distribution is a family of continuous probability distributions that are frequently used in hypothesis testing, particularly with categorical data. It's defined by its degrees of freedom and is often used to determine if there's a significant difference between observed and expected frequencies in one or more categories.\n",
        "Here's a more detailed explanation:\n",
        "What is it?\n",
        "A chi-square distribution is a probability distribution that arises when dealing with sums of squared standard normal random variables.\n",
        "It's skewed to the right, meaning it has a long tail on the positive side, and it only takes on non-negative values.\n",
        "The shape of the distribution is determined by its degrees of freedom, which is a parameter related to the number of independent variables involved.\n",
        "When is it used?\n",
        "The chi-square distribution is primarily used in hypothesis testing, specifically for:\n",
        "\n",
        "Goodness-of-fit tests:\n",
        "These tests determine how well observed data fits an expected distribution. For example, testing if the distribution of eye colors in a population matches a known distribution.\n",
        "\n",
        "Tests of independence:\n",
        "These tests assess whether there's a statistically significant relationship between two categorical variables. For example, determining if gender and preference for a product are independent or related.\n",
        "\n",
        "Tests of homogeneity:\n",
        "These tests compare the distribution of a categorical variable across different populations.\n",
        "Key points about chi-square tests:\n",
        "Chi-square tests are typically used with categorical data, where variables are classified into categories.\n",
        "The chi-square test statistic is calculated by comparing observed frequencies (what you actually see in your data) with expected frequencies (what you would expect if there were no relationship or difference).\n",
        "The larger the chi-square value, the greater the discrepancy between observed and expected values, and the more likely it is that the null hypothesis (which usually states there's no relationship or difference) will be rejected.\n"
      ],
      "metadata": {
        "id": "VaWmiRfY2RR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the Chi-square goodness of fit test, and how is it applied?\n",
        " - The chi-square goodness-of-fit test is a statistical hypothesis test used to determine how well a sample distribution matches an expected distribution. It assesses whether there's a significant difference between observed frequencies (what you actually see in your data) and expected frequencies (what you would anticipate based on a specific distribution). Essentially, it checks if the observed data \"fits\" the expected distribution.\n",
        "Here's how it's applied:\n",
        "Define the Hypothesis:\n",
        "Null Hypothesis (H0): The observed distribution matches the expected distribution.\n",
        "Alternative Hypothesis (Ha): The observed distribution does not match the expected distribution.\n",
        "Collect and Organize Data: Gather your observed data, grouping it into categories or intervals.\n",
        "Calculate Expected Frequencies: For each category, determine the expected frequency based on the hypothesized distribution.\n",
        "Calculate the Chi-Square Statistic: Use the formula: χ² = Σ [(Observed - Expected)² / Expected]. This formula calculates the sum of squared differences between observed and expected frequencies, divided by the expected frequencies.\n",
        "Determine Degrees of Freedom: Degrees of freedom (df) are calculated as (number of categories - 1).\n",
        "Find the P-value: Using the calculated chi-square statistic and degrees of freedom, find the corresponding p-value from a chi-square distribution table or statistical software.\n",
        "Make a Decision:\n",
        "If the p-value is less than your chosen significance level (alpha, often 0.05), reject the null hypothesis. This suggests a statistically significant difference between the observed and expected distributions.\n",
        "If the p-value is greater than or equal to the significance level, fail to reject the null hypothesis. This indicates that the observed data is consistent with the expected distribution."
      ],
      "metadata": {
        "id": "80u0bjXG2232"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What is the F-distribution, and when is it used in hypothesis testing?\n",
        " - The F-distribution is a probability distribution that is used in hypothesis testing, particularly when comparing variances or means of multiple groups. It's commonly used in analysis of variance (ANOVA) and in testing the significance of regression models. The F-distribution helps determine if observed differences between groups are statistically significant or likely due to random chance.\n",
        "Key Uses of the F-distribution in Hypothesis Testing:\n",
        "Analysis of Variance (ANOVA):\n",
        "ANOVA uses the F-distribution to test the null hypothesis that the means of two or more groups are equal. It compares the variance between groups to the variance within groups. If the F-statistic (calculated from the variances) is large enough, the null hypothesis is rejected, suggesting that at least one group mean is significantly different from the others.\n",
        "Comparing Variances:\n",
        "The F-distribution is used to test if two sample variances are significantly different from each other. This is done by calculating an F-statistic which is the ratio of the two sample variances.\n",
        "Regression Analysis:\n",
        "In regression, the F-test is used to assess the overall significance of the model. It compares the variance explained by the model to the unexplained variance (error). A significant F-statistic suggests that the model as a whole is a good fit for the data."
      ],
      "metadata": {
        "id": "Ze9433l53RB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is an ANOVA test, and what are its assumptions?\n",
        " - An ANOVA (Analysis of Variance) test is a statistical method used to compare the means of three or more groups to determine if there is a statistically significant difference between them. It essentially tests the null hypothesis that all group means are equal against the alternative hypothesis that at least one group mean is different. ANOVA is widely used in various fields, including biomedical research, to analyze data from experiments with multiple groups.\n",
        "Assumptions of ANOVA:\n",
        "To ensure the validity of ANOVA results, several key assumptions need to be met:\n",
        "1. Normality:\n",
        "The data within each group should be approximately normally distributed. This means that the data should follow a bell-shaped curve when plotted.\n",
        "2. Homogeneity of Variance:\n",
        "The variances (or standard deviations) of the groups being compared should be roughly equal. This is also known as homoscedasticity.\n",
        "3. Independence:\n",
        "The observations within each group and between groups should be independent of each other. This means that the data collected from one group should not influence the data collected from another group.\n",
        "Violating these assumptions can lead to inaccurate results, so it's important to check them before conducting an ANOVA test.\n"
      ],
      "metadata": {
        "id": "lXdPGswY30vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What are the different types of ANOVA tests?\n",
        " - The primary types of ANOVA tests are one-way ANOVA and two-way ANOVA. One-way ANOVA is used when there's one independent variable with multiple levels (groups) influencing a dependent variable. Two-way ANOVA is used when there are two independent variables, allowing analysis of their individual effects and their interaction on the dependent variable.\n",
        "Here's a breakdown of the two main types:\n",
        "\n",
        "One-Way ANOVA:\n",
        "Purpose: Compares the means of three or more groups based on a single independent variable.\n",
        "Example: Comparing the average test scores of students in three different teaching methods.\n",
        "Key feature: Only one independent variable is considered.\n",
        "\n",
        "Two-Way ANOVA:\n",
        "Purpose:\n",
        "Examines the effects of two independent variables and their interaction on a dependent variable.\n",
        "Example:\n",
        "Investigating the effect of both teaching method (one independent variable) and student gender (another independent variable) on test scores.\n",
        "Key feature:\n",
        "Allows analysis of individual (main) effects and the interaction effect between the two independent variables.\n",
        "Other ANOVA Variations:\n",
        "While one-way and two-way ANOVA are the most common, other variations exist, including:\n",
        "Repeated Measures ANOVA:\n",
        "Used when the same subjects are measured under different conditions or at different time points.\n",
        "Factorial ANOVA:\n",
        "A general term for ANOVA models with multiple independent variables (including two-way, three-way, etc.).\n",
        "N-way ANOVA:\n",
        "A generalization of two-way ANOVA where 'n' represents the number of independent variables.\n",
        "Ranked ANOVA:\n",
        "Used when data doesn't meet the assumptions of standard ANOVA, often with ordinal data.\n"
      ],
      "metadata": {
        "id": "yz0KwqDb4HVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. What is the F-test, and how does it relate to hypothesis testing?\n",
        " - The F-test is a statistical test that compares the variances of two or more samples or populations to determine if there's a significant difference between them. It's commonly used in hypothesis testing to assess whether the differences observed in sample variances are likely due to random chance or a true difference in the populations from which the samples were drawn.\n",
        "Here's a breakdown of the F-test and its relation to hypothesis testing:\n",
        "What is an F-test?\n",
        "Purpose:\n",
        "The primary purpose of an F-test is to compare variances. It's often used as a preliminary step before comparing means (using t-tests or ANOVA) to ensure the variances of the groups being compared are similar enough to justify the use of those tests.\n",
        "F-statistic:\n",
        "The F-test calculates an F-statistic, which is a ratio of two variances. The larger variance is typically placed in the numerator, and the smaller variance in the denominator.\n",
        "F-distribution:\n",
        "The F-statistic is then compared to a critical value from the F-distribution. The F-distribution is a family of curves that depend on the degrees of freedom for both the numerator and denominator in the F-statistic.\n",
        "How it Relates to Hypothesis Testing:\n",
        "Null Hypothesis:\n",
        "In an F-test, the null hypothesis typically states that the variances of the populations being compared are equal. The alternative hypothesis usually states that the variances are not equal.\n",
        "Decision:\n",
        "By comparing the calculated F-statistic to the critical value from the F-distribution (or by looking at the p-value associated with the F-statistic), we determine whether to reject the null hypothesis.\n",
        "Example:\n",
        "In a one-way ANOVA (analysis of variance), the F-test is used to compare the variances between multiple groups. If the F-test indicates that the variances are significantly different, then the ANOVA test can proceed to compare the means of those groups."
      ],
      "metadata": {
        "id": "LZxpo6cZ4Y_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Part - 1"
      ],
      "metadata": {
        "id": "fqs4z2l05Ev5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program to generate a random variable and display its value.\n",
        " - To generate a random variable and display its value in Python, the random module can be utilized. This module provides functions for generating pseudo-random numbers.\n",
        "Here is a Python program demonstrating how to generate and display a random integer:"
      ],
      "metadata": {
        "id": "LLvw1BLQ5IaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate a random integer between 1 and 10 (inclusive)\n",
        "random_number = random.randint(1, 10)\n",
        "\n",
        "# Display the generated random number\n",
        "print(f\"The random variable's value is: {random_number}\")"
      ],
      "metadata": {
        "id": "COs05H0a5bIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this program:\n",
        "The import random statement imports the necessary module.\n",
        "random.randint(1, 10) generates a random integer within the specified range, including both the lower and upper bounds.\n",
        "The generated value is assigned to the random_number variable.\n",
        "Finally, print() displays the value of random_number.\n",
        "Other functions within the random module can be used to generate different types of random variables, such as random.random() for a random float between 0.0 and 1.0, or random.choice() to select a random element from a sequence."
      ],
      "metadata": {
        "id": "EhgOCu7Z5c8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Generate a discrete uniform distribution using Python and plot the probability mass function (PMF).\n",
        " - To generate a discrete uniform distribution in Python and plot its Probability Mass Function (PMF), the scipy.stats module, specifically randint, is used. Matplotlib is then used for plotting."
      ],
      "metadata": {
        "id": "alhxYc645rNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Define the range for the discrete uniform distribution\n",
        "low_bound = 1  # Inclusive lower bound\n",
        "high_bound = 7  # Exclusive upper bound (so values are 1, 2, 3, 4, 5, 6)\n",
        "\n",
        "# Create an array of possible outcomes\n",
        "x = np.arange(low_bound, high_bound)\n",
        "\n",
        "# Calculate the Probability Mass Function (PMF) for each outcome\n",
        "pmf_values = randint.pmf(x, low=low_bound, high=high_bound)\n",
        "\n",
        "# Plot the PMF\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.stem(x, pmf_values, use_line_collection=True, markerfmt='o', linefmt='--')\n",
        "plt.title('Probability Mass Function of a Discrete Uniform Distribution')\n",
        "plt.xlabel('Outcome')\n",
        "plt.ylabel('Probability')\n",
        "plt.xticks(x)  # Ensure x-axis ticks are at each integer outcome\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aOwDcwFJ51Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "low_bound and high_bound define the range of the discrete uniform distribution. In scipy.stats.randint, high is exclusive, meaning the distribution includes integers from low up to high - 1.\n",
        "np.arange(low_bound, high_bound) creates an array of all possible integer outcomes.\n",
        "randint.pmf(x, low=low_bound, high=high_bound) calculates the probability for each outcome in x. For a discrete uniform distribution, each valid outcome has an equal probability, which is 1 / (high_bound - low_bound).\n",
        "plt.stem is used to plot the PMF, which is suitable for discrete distributions, showing a vertical line at each outcome representing its probability."
      ],
      "metadata": {
        "id": "fmKEeUIu5_fM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python function to calculate the probability distribution function (PDF) of a Bernoulli distribution."
      ],
      "metadata": {
        "id": "br22hoPx6ARP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import bernoulli\n",
        "\n",
        "def bernoulli_pdf(k, p):\n",
        "  \"\"\"\n",
        "  Calculates the probability mass function (PMF) of a Bernoulli distribution.\n",
        "\n",
        "  Args:\n",
        "      k: The outcome (0 or 1).\n",
        "      p: The probability of success (outcome 1).\n",
        "\n",
        "  Returns:\n",
        "      The probability of the given outcome (P(X=k)).\n",
        "  \"\"\"\n",
        "  return bernoulli.pmf(k, p)\n",
        "\n",
        "# Example usage\n",
        "probability_of_success = 0.7\n",
        "outcome = 1\n",
        "\n",
        "pdf_value = bernoulli_pdf(outcome, probability_of_success)\n",
        "print(f\"Probability of outcome {outcome} with p={probability_of_success}: {pdf_value}\")\n",
        "\n",
        "outcome = 0\n",
        "pdf_value = bernoulli_pdf(outcome, probability_of_success)\n",
        "print(f\"Probability of outcome {outcome} with p={probability_of_success}: {pdf_value}\")"
      ],
      "metadata": {
        "id": "EWsAKPOR6Jg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bernoulli_pdf function utilizes the scipy.stats.bernoulli.pmf function, which directly calculates the probability mass function (PMF) of a Bernoulli distribution. The function takes the outcome (k) and the probability of success (p) as input and returns the probability of that outcome. The example demonstrates how to use the function to calculate the probability for both outcomes (0 and 1) given a specific probability of success."
      ],
      "metadata": {
        "id": "N9HkoBI76KNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python script to simulate a binomial distribution with n=10 and p=0.5, then plot its histogram.\n",
        " - The following Python script simulates a binomial distribution with parameters \\(n=10\\) (number of trials) and \\(p=0.5\\) (probability of success), and then visualizes its histogram using matplotlib."
      ],
      "metadata": {
        "id": "IH5AHfQs6NP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the parameters of the binomial distribution\n",
        "n = 10  # Number of trials\n",
        "p = 0.5 # Probability of success\n",
        "\n",
        "# Generate random samples from the binomial distribution\n",
        "# 'size' determines how many samples to generate for the simulation\n",
        "num_samples = 10000\n",
        "data = np.random.binomial(n=n, p=p, size=num_samples)\n",
        "\n",
        "# Create a histogram of the simulated data\n",
        "plt.hist(data, bins=np.arange(-0.5, n + 1.5, 1), density=True, edgecolor='black', alpha=0.7)\n",
        "\n",
        "# Add labels and title to the plot\n",
        "plt.title(f'Simulated Binomial Distribution (n={n}, p={p})')\n",
        "plt.xlabel('Number of Successes')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.xticks(range(n + 1)) # Ensure integer ticks for number of successes\n",
        "\n",
        "# Display the plot\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zxctjOQF6YzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Create a Poisson distribution and visualize it using Python.\n",
        " - A Poisson distribution can be created and visualized in Python using the numpy and matplotlib libraries. Generate Poisson Distributed Data.             Use numpy.random.poisson() to generate random samples following a Poisson distribution. The lam parameter represents the expected number of events in an interval (lambda, \\(\\lambda \\)), and size specifies the number of samples to generate."
      ],
      "metadata": {
        "id": "oGjl20Ll6bJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    import numpy as np\n",
        "\n",
        "    # Set the lambda (mean rate of events)\n",
        "    lam = 3\n",
        "\n",
        "    # Generate 1000 random samples from a Poisson distribution\n",
        "    poisson_data = np.random.poisson(lam=lam, size=1000)"
      ],
      "metadata": {
        "id": "_aQhlnm16zxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the Distribution.\n",
        "Use matplotlib.pyplot.hist() to create a histogram of the generated data. This visually represents the probability mass function of the Poisson distribution."
      ],
      "metadata": {
        "id": "VqB2nN3S61r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Create a histogram\n",
        "    plt.hist(poisson_data, bins=np.arange(0, max(poisson_data) + 2) - 0.5, density=True, rwidth=0.8, color='skyblue', edgecolor='black')\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel(\"Number of Events (k)\")\n",
        "    plt.ylabel(\"Probability\")\n",
        "    plt.title(f\"Poisson Distribution (λ = {lam})\")\n",
        "    plt.xticks(np.arange(0, max(poisson_data) + 1))\n",
        "    plt.grid(axis='y', alpha=0.75)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "g0Vtqq9q65xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to calculate and plot the cumulative distribution function (CDF) of a discrete\n",
        "uniform distribution.\n",
        " - The following Python program calculates and plots the Cumulative Distribution Function (CDF) of a discrete uniform distribution."
      ],
      "metadata": {
        "id": "OqOpElDy68N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def discrete_uniform_cdf(x, a, b):\n",
        "    \"\"\"\n",
        "    Calculates the CDF of a discrete uniform distribution.\n",
        "\n",
        "    Args:\n",
        "        x (int or float): The value at which to evaluate the CDF.\n",
        "        a (int): The lower bound of the discrete uniform distribution (inclusive).\n",
        "        b (int): The upper bound of the discrete uniform distribution (inclusive).\n",
        "\n",
        "    Returns:\n",
        "        float: The CDF value at x.\n",
        "    \"\"\"\n",
        "    if not (isinstance(a, int) and isinstance(b, int) and a <= b):\n",
        "        raise ValueError(\"a and b must be integers with a <= b.\")\n",
        "\n",
        "    if x < a:\n",
        "        return 0\n",
        "    elif a <= x <= b:\n",
        "        return (np.floor(x) - a + 1) / (b - a + 1)\n",
        "    else:  # x > b\n",
        "        return 1\n",
        "\n",
        "# Define the parameters of the discrete uniform distribution\n",
        "lower_bound = 1\n",
        "upper_bound = 10\n",
        "\n",
        "# Generate a range of x values for plotting\n",
        "x_values = np.linspace(lower_bound - 2, upper_bound + 2, 500)\n",
        "\n",
        "# Calculate the CDF for each x value\n",
        "cdf_values = [discrete_uniform_cdf(val, lower_bound, upper_bound) for val in x_values]\n",
        "\n",
        "# Plot the CDF\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_values, cdf_values, drawstyle='steps-post', label='CDF of Discrete Uniform Distribution')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('F(x)')\n",
        "plt.title(f'Cumulative Distribution Function of a Discrete Uniform Distribution (a={lower_bound}, b={upper_bound})')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O5sH4gVP7MFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Generate a continuous uniform distribution using NumPy and visualize it.\n",
        " - To generate a continuous uniform distribution using NumPy and visualize it, follow these steps:\n",
        "Generate Samples:\n",
        "Use numpy.random.uniform() to generate random samples from a specified continuous uniform distribution. You can define the lower bound (low), upper bound (high), and the number of samples (size).\n",
        "Visualize the Distribution:\n",
        "Use a plotting library like Matplotlib to visualize the distribution. A histogram is a suitable choice for visualizing continuous distributions as it shows the frequency of values within different bins, which for a uniform distribution should appear relatively flat across the defined range.\n",
        "Here is an example:"
      ],
      "metadata": {
        "id": "QtC4pxVX7Obi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Generate Samples from a Continuous Uniform Distribution\n",
        "# Parameters: low=0 (inclusive), high=10 (exclusive), size=1000 samples\n",
        "uniform_samples = np.random.uniform(low=0, high=10, size=10000)\n",
        "\n",
        "# 2. Visualize the Distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(uniform_samples, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.title('Continuous Uniform Distribution')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m3cI-vpm7XA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Simulate data from a normal distribution and plot its histogram.\n",
        " - To simulate data from a normal distribution and create a histogram, you can use Python libraries like NumPy for generating random numbers and Matplotlib for plotting. First, generate a set of random samples from a normal distribution using np.random.normal(). Then, use plt.hist() to create a histogram of the generated data. Optionally, you can overlay a probability density function (PDF) of the normal distribution on the histogram using plt.plot() with norm.pdf()."
      ],
      "metadata": {
        "id": "bOBLrBS87ZhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Parameters for the normal distribution\n",
        "mean = 0\n",
        "std_dev = 1\n",
        "num_samples = 1000\n",
        "\n",
        "# Generate random samples\n",
        "data = np.random.normal(mean, std_dev, num_samples)\n",
        "\n",
        "# Create the histogram\n",
        "plt.hist(data, bins=30, density=True, alpha=0.6, color='skyblue', label='Histogram')\n",
        "\n",
        "# Overlay the normal distribution curve\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = norm.pdf(x, mean, std_dev)\n",
        "plt.plot(x, p, 'k', linewidth=2, label='Normal Distribution')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency/Probability')\n",
        "plt.title('Normal Distribution Histogram')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Am93WTZd7mdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet first sets the parameters for the normal distribution (mean and standard deviation) and the number of samples to generate. Then, it uses np.random.normal() to create the random data. Next, plt.hist() creates the histogram, with density=True to normalize the histogram to represent probability density. The normal distribution is overlaid by first calculating the PDF using norm.pdf() and then plotting it. Finally, labels, a title, and a legend are added to the plot for clarity."
      ],
      "metadata": {
        "id": "5R-TLwdu7qBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python function to calculate Z-scores from a dataset and plot them.\n",
        " - A Python function can be created to calculate Z-scores from a dataset and visualize them using a histogram."
      ],
      "metadata": {
        "id": "PSR4ys0L7tFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import zscore\n",
        "\n",
        "def calculate_and_plot_zscores(data, title=\"Z-score Distribution\"):\n",
        "    \"\"\"\n",
        "    Calculates Z-scores for a given dataset and plots their distribution.\n",
        "\n",
        "    Args:\n",
        "        data (array-like): The input dataset (e.g., a list or NumPy array).\n",
        "        title (str, optional): The title for the plot. Defaults to \"Z-score Distribution\".\n",
        "    \"\"\"\n",
        "    if not isinstance(data, (list, np.ndarray)):\n",
        "        raise TypeError(\"Input data must be a list or a NumPy array.\")\n",
        "\n",
        "    # Convert data to a NumPy array for consistent operations\n",
        "    data_array = np.array(data)\n",
        "\n",
        "    # Calculate Z-scores\n",
        "    z_scores = zscore(data_array)\n",
        "\n",
        "    # Plot the Z-score distribution\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(z_scores, bins=20, edgecolor='black', alpha=0.7)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Z-score\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.axvline(0, color='red', linestyle='--', label='Mean (Z=0)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return z_scores\n",
        "\n",
        "# Example Usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample dataset\n",
        "    sample_data = [10, 12, 15, 18, 20, 22, 25, 28, 30, 35, 40, 5, 50]\n",
        "\n",
        "    # Calculate and plot Z-scores\n",
        "    calculated_z_scores = calculate_and_plot_zscores(sample_data, \"Distribution of Z-scores for Sample Data\")\n",
        "    print(\"\\nCalculated Z-scores:\")\n",
        "    print(calculated_z_scores)\n",
        "\n",
        "    # Another example with a different dataset\n",
        "    another_data = np.random.normal(loc=100, scale=15, size=500)\n",
        "    calculate_and_plot_zscores(another_data, \"Distribution of Z-scores for Normally Distributed Data\")"
      ],
      "metadata": {
        "id": "JIqI1ROl8BTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Implement the Central Limit Theorem (CLT) using Python for a non-normal distribution.\n",
        " - Here’s a robust Python implementation demonstrating the Central Limit Theorem (CLT) using a highly non-normal exponential distribution. You'll see how sample means converge toward a normal distribution even though the population is skewed:"
      ],
      "metadata": {
        "id": "lHcQtaaf8EZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# 1. Simulate a skewed population (exponential distribution)\n",
        "np.random.seed(0)\n",
        "population = np.random.exponential(scale=2.0, size=100_000)  # cf. geeksforgeeks example :contentReference[oaicite:1]{index=1}\n",
        "\n",
        "# 2. Set sampling parameters\n",
        "sample_size = 50       # observations per sample\n",
        "num_samples = 1_000    # total number of samples\n",
        "\n",
        "# 3. Compute means of repeated samples\n",
        "sample_means = [\n",
        "    np.mean(np.random.choice(population, size=sample_size, replace=True))\n",
        "    for _ in range(num_samples)\n",
        "]\n",
        "\n",
        "# 4. Plot histogram of sample means and overlay fitted normal\n",
        "mu, sigma = np.mean(sample_means), np.std(sample_means, ddof=1)\n",
        "plt.hist(sample_means, bins=40, density=True, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 200)\n",
        "plt.plot(x, norm.pdf(x, mu, sigma), 'r--', lw=2)\n",
        "plt.title(f\"CLT: Sample size = {sample_size}, #samples = {num_samples}\")\n",
        "plt.xlabel(\"Sample mean\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oTFfRKcL8jWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔍 Explanation & Citations\n",
        "The population is clearly non-normal—it's drawn from an exponential distribution (scale=2.0), as shown in similar CLT demos\n",
        "gulinan.github.io\n",
        "+4\n",
        "geeksforgeeks.org\n",
        "+4\n",
        "rajivgopinath.com\n",
        "+4\n",
        "intro.quantecon.org\n",
        "+2\n",
        "datascienzz.com\n",
        "+2\n",
        "koshurai.medium.com\n",
        "+2\n",
        "intro.quantecon.org\n",
        ".\n",
        "\n",
        "We draw 1,000 samples of size 50 each, computing their means.\n",
        "\n",
        "Plotting the histogram of these means reveals an approximate normal (bell-curve) shape, consistent with CLT predictions\n",
        "koshurai.medium.com\n",
        ".\n",
        "\n",
        "The red dashed line is the theoretical normal distribution with mean µ and standard error σ. You’ll see it aligns well with the sample-mean histogram.\n",
        "\n",
        "✅ Summary\n",
        "Even with a skewed population, sample means converge to a normal distribution as sample size increases—that’s the essence of the CLT\n",
        "\n",
        "The simulation above clearly visualizes this effect."
      ],
      "metadata": {
        "id": "BWwpJsM18mvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Simulate multiple samples from a normal distribution and verify the Central Limit Theorem.\n",
        " - The Central Limit Theorem (CLT) can be demonstrated by simulating multiple samples from a distribution, calculating the mean of each sample, and observing that the distribution of these sample means tends towards a normal distribution, regardless of the original distribution's shape, provided the sample size is sufficiently large.\n",
        "Here's a Python code example that illustrates this:"
      ],
      "metadata": {
        "id": "QqDnuYQB82rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set parameters\n",
        "num_samples = 10000\n",
        "sample_size = 30\n",
        "original_distribution_mean = 5\n",
        "original_distribution_std = 2\n",
        "\n",
        "# Create a skewed distribution (e.g., exponential)\n",
        "# You can change this to other distributions (uniform, etc.)\n",
        "original_distribution = np.random.exponential(scale=original_distribution_mean, size=num_samples)\n",
        "\n",
        "# Store sample means\n",
        "sample_means = []\n",
        "\n",
        "# Simulate multiple samples and calculate their means\n",
        "for _ in range(num_samples):\n",
        "    # Generate a sample from the original distribution\n",
        "    sample = np.random.choice(original_distribution, size=sample_size, replace=True)\n",
        "    # Calculate the mean of the sample\n",
        "    sample_mean = np.mean(sample)\n",
        "    # Store the sample mean\n",
        "    sample_means.append(sample_mean)\n",
        "\n",
        "# Plot the original distribution and the distribution of sample means\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Original distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(original_distribution, bins=50, density=True, alpha=0.6, color='blue', label='Original Distribution')\n",
        "plt.title('Original Distribution (e.g., Exponential)')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "\n",
        "# Distribution of sample means\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(sample_means, bins=50, density=True, alpha=0.6, color='red', label='Sample Means')\n",
        "plt.title('Distribution of Sample Means (CLT)')\n",
        "plt.xlabel('Mean Value')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# Overlay a normal distribution for comparison\n",
        "x = np.linspace(min(sample_means), max(sample_means), 100)\n",
        "plt.plot(x, 1/(np.std(sample_means) * np.sqrt(2 * np.pi)) * np.exp( - (x - np.mean(sample_means))**2 / (2 * np.std(sample_means)**2) ),\n",
        "         linewidth=2, color='green', label='Normal Distribution')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "12_ttBco9Gyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "An original distribution is created (here an exponential distribution, but can be changed to uniform, etc.).\n",
        "Multiple samples of a specified size are drawn from this distribution.\n",
        "The mean of each sample is calculated.\n",
        "The distribution of these sample means is plotted. A normal distribution is overlaid for comparison.\n",
        "The plot demonstrates that the distribution of sample means approaches a normal distribution, even though the original distribution was not normal. This verifies the Central Limit Theorem. According to Scribbr."
      ],
      "metadata": {
        "id": "QG1eigXK9Jkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python function to calculate and plot the standard normal distribution (mean = 0, std = 1).\n",
        " - Here's a concise Python function to calculate and plot the standard normal distribution (mean = 0, std = 1) using numpy, matplotlib, and scipy.stats:"
      ],
      "metadata": {
        "id": "VPLuML449NFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "def plot_standard_normal(x_min=-4, x_max=4, num_points=1000):\n",
        "    \"\"\"\n",
        "    Plots the standard normal distribution curve (mean=0, std=1).\n",
        "\n",
        "    Parameters:\n",
        "    - x_min, x_max: range of x-axis values\n",
        "    - num_points: resolution of the curve\n",
        "    \"\"\"\n",
        "    x = np.linspace(x_min, x_max, num_points)\n",
        "    y = norm.pdf(x, loc=0, scale=1)\n",
        "\n",
        "    plt.plot(x, y, 'b-', label='Standard Normal PDF')\n",
        "    plt.title('Standard Normal Distribution (μ=0, σ=1)')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('Probability Density')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    plot_standard_normal()\n"
      ],
      "metadata": {
        "id": "GmqA199E9qB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 How this works\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "Creates 1,000 evenly spaced points between –4 and 4, enough to cover the central bulk of the distribution.\n",
        "how2matplotlib.com\n",
        "+14\n",
        "geeksforgeeks.org\n",
        "+14\n",
        "vitalflux.com\n",
        "+14\n",
        "\n",
        "norm.pdf(x, loc=0, scale=1)\n",
        "Computes the probability density function (PDF) of the standard normal distribution.\n",
        "geeksforgeeks.org\n",
        "\n",
        "plt.plot(x, y, …)\n",
        "Draws a smooth, bell‑shaped curve. Using 'b-' makes it a solid blue line.\n",
        "\n",
        "✅ What you can customize\n",
        "x_min, x_max – expand or shrink the x‑range to capture more of the tails.\n",
        "\n",
        "num_points – increase for a smoother line, or reduce for performance.\n",
        "\n",
        "loc, scale inside norm.pdf – set these for other normal distributions (non-standard).\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfYeRHe_9ywQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Generate random variables and calculate their corresponding probabilities using the binomial distribution.\n",
        " - To calculate probabilities using the binomial distribution, you need to define the number of trials (n), the probability of success in a single trial (p), and the number of successes you're interested in (k). The binomial probability formula then calculates the probability of exactly k successes in n trials.\n",
        "Here's how to calculate binomial probabilities:\n",
        "\n",
        "Understand the Binomial Distribution:\n",
        "The binomial distribution models the probability of a specific number of successes in a fixed number of independent trials, where each trial has only two possible outcomes (success or failure) with a constant probability of success.\n",
        "Key parameters:\n",
        "n: The number of trials.\n",
        "p: The probability of success in a single trial.\n",
        "k: The number of successes we are interested in (0 <= k <= n).\n",
        "\n",
        "The Binomial Probability Formula:\n",
        "The probability of exactly k successes in n trials is given by:\n",
        "P(X = k) = C(n, k) * p^k * (1-p)^(n-k)\n",
        "Where:\n",
        "P(X = k) is the probability of exactly k successes.\n",
        "C(n, k) is the binomial coefficient, which represents the number of combinations of n items taken k at a time, calculated as n! / (k! * (n-k)!).\n",
        "p^k is the probability of k successes.\n",
        "(1-p)^(n-k) is the probability of n-k failures.\n",
        "\n",
        "Example:\n",
        "Let's say you flip a fair coin 5 times (n=5) and want to know the probability of getting exactly 3 heads (k=3). The probability of getting heads on a single flip is 0.5 (p=0.5).\n",
        "C(5, 3) = 5! / (3! * 2!) = 10.\n",
        "p^k = (0.5)^3 = 0.125.\n",
        "(1-p)^(n-k) = (0.5)^2 = 0.25.\n",
        "P(X = 3) = 10 * 0.125 * 0.25 = 0.3125.\n",
        "Therefore, the probability of getting exactly 3 heads in 5 coin flips is 0.3125.\n",
        "\n",
        "Using Python for Calculation:"
      ],
      "metadata": {
        "id": "NCZHQM-I985i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def binomial_probability(n, k, p):\n",
        "  \"\"\"\n",
        "  Calculates the binomial probability.\n",
        "\n",
        "  Args:\n",
        "    n: The number of trials.\n",
        "    k: The number of successes.\n",
        "    p: The probability of success in a single trial.\n",
        "\n",
        "  Returns:\n",
        "    The binomial probability P(X=k).\n",
        "  \"\"\"\n",
        "  combinations = math.comb(n, k) # or math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n",
        "  probability = combinations * (p**k) * ((1-p)**(n-k))\n",
        "  return probability\n",
        "\n",
        "# Example usage:\n",
        "n_trials = 5\n",
        "num_successes = 3\n",
        "probability_of_success = 0.5\n",
        "\n",
        "probability = binomial_probability(n_trials, num_successes, probability_of_success)\n",
        "print(f\"The probability of {num_successes} successes in {n_trials} trials is: {probability}\")"
      ],
      "metadata": {
        "id": "4BDExF0r_mfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to calculate the Z-score for a given data point and compare it to a standard normal\n",
        "distribution.\n",
        " - A Python program for calculating the Z-score of a data point and comparing it to a standard normal distribution is provided below."
      ],
      "metadata": {
        "id": "iWcLLHuM_otU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "def calculate_z_score(data_point, mean, std_dev):\n",
        "    \"\"\"\n",
        "    Calculates the Z-score for a given data point.\n",
        "\n",
        "    Args:\n",
        "        data_point (float): The individual data point.\n",
        "        mean (float): The mean of the dataset.\n",
        "        std_dev (float): The standard deviation of the dataset.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated Z-score.\n",
        "    \"\"\"\n",
        "    if std_dev == 0:\n",
        "        raise ValueError(\"Standard deviation cannot be zero.\")\n",
        "    z_score = (data_point - mean) / std_dev\n",
        "    return z_score\n",
        "\n",
        "def compare_to_standard_normal(z_score):\n",
        "    \"\"\"\n",
        "    Compares the calculated Z-score to a standard normal distribution.\n",
        "\n",
        "    Args:\n",
        "        z_score (float): The calculated Z-score.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - float: The cumulative probability (P-value) of observing a value\n",
        "                     less than or equal to the given Z-score in a standard\n",
        "                     normal distribution.\n",
        "            - str: A descriptive statement about the Z-score's position\n",
        "                   relative to the mean of the standard normal distribution.\n",
        "    \"\"\"\n",
        "    cumulative_probability = norm.cdf(z_score)\n",
        "\n",
        "    if z_score > 0:\n",
        "        position_description = \"The data point is above the mean of the standard normal distribution.\"\n",
        "    elif z_score < 0:\n",
        "        position_description = \"The data point is below the mean of the standard normal distribution.\"\n",
        "    else:\n",
        "        position_description = \"The data point is at the mean of the standard normal distribution.\"\n",
        "\n",
        "    return cumulative_probability, position_description\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example Usage:\n",
        "    data_point_val = 75\n",
        "    dataset_mean = 70\n",
        "    dataset_std_dev = 5\n",
        "\n",
        "    # Calculate Z-score\n",
        "    try:\n",
        "        z = calculate_z_score(data_point_val, dataset_mean, dataset_std_dev)\n",
        "        print(f\"The Z-score for data point {data_point_val} is: {z:.2f}\")\n",
        "\n",
        "        # Compare to standard normal distribution\n",
        "        prob, description = compare_to_standard_normal(z)\n",
        "        print(f\"Cumulative probability (P-value) for Z-score {z:.2f}: {prob:.4f}\")\n",
        "        print(description)\n",
        "\n",
        "        # Another example\n",
        "        data_point_val_neg = 60\n",
        "        z_neg = calculate_z_score(data_point_val_neg, dataset_mean, dataset_std_dev)\n",
        "        print(f\"\\nThe Z-score for data point {data_point_val_neg} is: {z_neg:.2f}\")\n",
        "        prob_neg, description_neg = compare_to_standard_normal(z_neg)\n",
        "        print(f\"Cumulative probability (P-value) for Z-score {z_neg:.2f}: {prob_neg:.4f}\")\n",
        "        print(description_neg)\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "KR1LwLGrANvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Implement hypothesis testing using Z-statistics for a sample dataset.\n",
        " - To implement hypothesis testing using Z‑statistics (when the population standard deviation is known and n ≥ 30 or population is normal), here’s a detailed step-by-step guide plus Python code to illustrate it using a sample dataset.\n",
        "\n",
        "🔍  Theory Overview\n",
        "Test statistic for a one-sample Z-test:\n",
        "𝑍\n",
        "=\n",
        "𝑥\n",
        "ˉ\n",
        "−\n",
        "𝜇\n",
        "0\n",
        "𝜎\n",
        "/\n",
        "𝑛\n",
        "Z=\n",
        "σ/\n",
        "n\n",
        "​\n",
        "\n",
        "x\n",
        "ˉ\n",
        " −μ\n",
        "0\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "\n",
        "𝑥\n",
        "ˉ\n",
        "x\n",
        "ˉ\n",
        " : sample mean\n",
        "\n",
        "𝜇\n",
        "0\n",
        "μ\n",
        "0\n",
        "​\n",
        " : hypothesized population mean\n",
        "\n",
        "𝜎\n",
        "σ: known population standard deviation\n",
        "\n",
        "𝑛\n",
        "n: sample size\n",
        "investopedia.com\n",
        "+6\n",
        "geeksforgeeks.org\n",
        "+6\n",
        "reddit.com\n",
        "+6\n",
        "cloud.r-project.org\n",
        "en.wikipedia.org\n",
        "+7\n",
        "statisticsuniverse.com\n",
        "+7\n",
        "bookdown.org\n",
        "+7\n",
        "scmathuitmkedah.github.io\n",
        "\n",
        "Decision rules:\n",
        "\n",
        "Two-tailed test at significance level\n",
        "𝛼\n",
        "α: reject\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        "  if\n",
        "∣\n",
        "𝑍\n",
        "∣\n",
        ">\n",
        "𝑍\n",
        "1\n",
        "−\n",
        "𝛼\n",
        "/\n",
        "2\n",
        "∣Z∣>Z\n",
        "1−α/2\n",
        "​\n",
        "  (e.g. 1.96 at 5%)\n",
        "\n",
        "Compute p-value as\n",
        "2\n",
        "×\n",
        "(\n",
        "1\n",
        "−\n",
        "Φ\n",
        "(\n",
        "∣\n",
        "𝑍\n",
        "∣\n",
        ")\n",
        ")\n",
        "2×(1−Φ(∣Z∣)) and compare to\n",
        "𝛼\n",
        "α\n",
        "ncl.ac.uk\n",
        "scmathuitmkedah.github.io\n",
        "\n",
        "🏫 2. Example with a Sample Dataset\n",
        "Suppose:\n",
        "\n",
        "𝑛\n",
        "=\n",
        "50\n",
        "n=50\n",
        "\n",
        "Sample mean\n",
        "𝑥\n",
        "ˉ\n",
        "=\n",
        "172.5\n",
        "x\n",
        "ˉ\n",
        " =172.5\n",
        "\n",
        "Population mean\n",
        "𝜇\n",
        "0\n",
        "=\n",
        "170\n",
        "μ\n",
        "0\n",
        "​\n",
        " =170\n",
        "\n",
        "Population SD\n",
        "𝜎\n",
        "=\n",
        "10\n",
        "σ=10\n",
        "\n",
        "Test at\n",
        "𝛼\n",
        "=\n",
        "0.05\n",
        "α=0.05 (two-tailed)\n",
        "\n",
        "Calculate:\n",
        "𝑍\n",
        "=\n",
        "172.5\n",
        "−\n",
        "170\n",
        "10\n",
        "/\n",
        "50\n",
        "≈\n",
        "1.77\n",
        "Z=\n",
        "10/\n",
        "50\n",
        "​\n",
        "\n",
        "172.5−170\n",
        "​\n",
        " ≈1.77\n",
        "\n",
        "From standard normal tables,\n",
        "∣\n",
        "𝑍\n",
        "∣\n",
        "=\n",
        "1.77\n",
        ">\n",
        "1.96\n",
        "∣Z∣=1.77>1.96? No → so we fail to reject\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " .\n",
        "Actually, the standard example shows p-value ≈ 0.0385 < 0.05 → reject\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " , concluding the mean is significantly different\n",
        "statisticsuniverse.com\n",
        "\n",
        "🧮 Python Implementation\n",
        "(a) Manual Z-test calculation"
      ],
      "metadata": {
        "id": "AJjQyR26ARDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Given values\n",
        "x_bar = 172.5\n",
        "mu0 = 170\n",
        "sigma = 10\n",
        "n = 50\n",
        "alpha = 0.05\n",
        "\n",
        "# Z-statistic\n",
        "z = (x_bar - mu0) / (sigma / math.sqrt(n))\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "\n",
        "print(f\"Z = {z:.2f}\")\n",
        "print(f\"p‑value = {p_value:.4f}\")\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"→ Reject H0: sample mean is significantly different.\")\n",
        "else:\n",
        "    print(\"→ Fail to reject H0: no significant difference.\")\n"
      ],
      "metadata": {
        "id": "SlNjFI4xDC3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Using statsmodels"
      ],
      "metadata": {
        "id": "L5XTCnz4DJ2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from statsmodels.stats.weightstats import ztest\n",
        "\n",
        "# Simulating a sample (for demo)\n",
        "data = np.random.normal(loc=172.5, scale=sigma, size=n)\n",
        "\n",
        "z_stat, p_val = ztest(data, value=mu0, alternative='two-sided')\n",
        "print(f\"Z = {z_stat:.2f}, p‑value = {p_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "aGZcxGXcDKwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧪 Full Example Workflow Summary\n",
        "Define hypotheses\n",
        "𝐻\n",
        "0\n",
        ":\n",
        "𝜇\n",
        "=\n",
        "170\n",
        "H\n",
        "0\n",
        "​\n",
        " :μ=170 vs\n",
        "𝐻\n",
        "𝑎\n",
        ":\n",
        "𝜇\n",
        "≠\n",
        "170\n",
        "H\n",
        "a\n",
        "​\n",
        " :μ\n",
        "\n",
        "=170\n",
        "\n",
        "Check assumptions\n",
        "\n",
        "Population σ known: ✔\n",
        "\n",
        "n ≥ 30: ✔ → Z-test appropriate\n",
        "support.minitab.com\n",
        "+15\n",
        "investopedia.com\n",
        "+15\n",
        "statisticsuniverse.com\n",
        "+15\n",
        "cloud.r-project.org\n",
        "+5\n",
        "en.wikipedia.org\n",
        "+5\n",
        "scmathuitmkedah.github.io\n",
        "+5\n",
        "reddit.com\n",
        "+5\n",
        "geeksforgeeks.org\n",
        "+5\n",
        "reddit.com\n",
        "+5\n",
        "cloud.r-project.org\n",
        "+3\n",
        "statisticsuniverse.com\n",
        "+3\n",
        "bookdown.org\n",
        "+3\n",
        "\n",
        "Calculate test statistic and p-value\n",
        "\n",
        "Decision\n",
        "Based on p-value vs\n",
        "𝛼\n",
        "α or Z vs critical value:"
      ],
      "metadata": {
        "id": "PmYgYoB8DW8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z = 1.77, p ≈ 0.0385 < 0.05 → reject H0\n"
      ],
      "metadata": {
        "id": "-JlMFvFWDbaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation\n",
        "The sample provides statistically significant evidence that the true mean is different from 170.\n",
        "\n",
        "🔧 When to Use Z vs T\n",
        "Use Z-test only when population standard deviation is known (or large n and substitute σ).\n",
        "\n",
        "Otherwise use a t-test (with sample standard deviation and degrees of freedom)\n",
        "\n",
        "📌  Quick Template You Can Adjust"
      ],
      "metadata": {
        "id": "-jMCfqKHDjat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_sample_z_test(sample_mean, mu0, sigma, n, alpha=0.05, alternative='two-sided'):\n",
        "    import math\n",
        "    import scipy.stats as stats\n",
        "\n",
        "    z = (sample_mean - mu0) / (sigma / math.sqrt(n))\n",
        "    if alternative == 'two-sided':\n",
        "        p = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "    elif alternative == 'larger':\n",
        "        p = 1 - stats.norm.cdf(z)\n",
        "    else:  # smaller\n",
        "        p = stats.norm.cdf(z)\n",
        "\n",
        "    crit = stats.norm.ppf(1 - alpha/2) if alternative == 'two-sided' else stats.norm.ppf(1 - alpha)\n",
        "\n",
        "    return z, p, crit\n"
      ],
      "metadata": {
        "id": "qz5pFmU0DpgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Create a confidence interval for a dataset using Python and interpret the result.\n",
        " - Here’s a step-by-step guide to calculate and interpret a confidence interval for a sample dataset in Python:\n",
        "\n",
        "🧮 Python Code Example (95% CI for the mean)"
      ],
      "metadata": {
        "id": "dbE3w_67DsdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=30, scale=5, size=100)\n",
        "\n",
        "# Calculate sample statistics\n",
        "sample_mean = np.mean(data)\n",
        "sample_std = np.std(data, ddof=1)\n",
        "n = len(data)\n",
        "confidence = 0.95\n",
        "\n",
        "# Critical z-value for 95% two-tailed\n",
        "z = stats.norm.ppf((1 + confidence) / 2)\n",
        "\n",
        "# Margin of error\n",
        "moe = z * (sample_std / np.sqrt(n))\n",
        "\n",
        "# Confidence interval\n",
        "ci_lower = sample_mean - moe\n",
        "ci_upper = sample_mean + moe\n",
        "\n",
        "print(f\"Sample mean = {sample_mean:.2f}\")\n",
        "print(f\"95% CI = ({ci_lower:.2f}, {ci_upper:.2f})\")\n"
      ],
      "metadata": {
        "id": "oGqyR1C8D9Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example draws 100 random observations from a normal distribution (mean ≈ 30, SD ≈ 5) and constructs a 95% confidence interval using the z‑distribution—appropriate for larger sample sizes"
      ],
      "metadata": {
        "id": "1TQJSWjmEB_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Interpretation\n",
        "Output: Suppose it prints:"
      ],
      "metadata": {
        "id": "b8VbT4fyEGsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sample mean = 30.10\n",
        "95% CI = (29.12, 31.08)\n"
      ],
      "metadata": {
        "id": "3Vby7DwBEITh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What it means: We are 95% confident the true population mean lies between 29.12 and 31.08.\n",
        "\n",
        "Frequentist nuance: If we repeated this sampling process many times, about 95% of the resulting intervals would contain the true mean\n",
        "\n",
        "Interval width: Reflects uncertainty—wider interval → more uncertainty. It narrows as n increases or σ decrease"
      ],
      "metadata": {
        "id": "YzEKR9fQEQJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔍 3. When to Use t vs z\n",
        "Use z-interval (normal distribution) if:\n",
        "\n",
        "Sample is large (typically n ≥ 30) and\n",
        "\n",
        "Population standard deviation is known—or sample SD approximates well (Central Limit Theorem)\n",
        ".\n",
        "\n",
        "Use t-interval when:\n",
        "\n",
        "Sample size is small (n < 30)\n",
        "\n",
        "Population SD is unknown\n",
        "Example with scipy.stats.t.interval(...)\n",
        "\n",
        "⚙️  Variant: Using SciPy Built-in"
      ],
      "metadata": {
        "id": "zPYs9y8wEY9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "data = [45, 55, 67, 45, 68, 79, 98, 87, 84, 82]  # small sample\n",
        "ci = stats.t.interval(0.95, df=len(data)-1, loc=np.mean(data), scale=stats.sem(data))\n",
        "print(f\"95% CI (t) = {ci}\")\n"
      ],
      "metadata": {
        "id": "Y14qUQ69Eh98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This uses the Student’s t-distribution when the sample is small and population SD is unknown"
      ],
      "metadata": {
        "id": "c2PRTMJLEoiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Generate data from a normal distribution, then calculate and interpret the confidence interval for its mean.\n",
        " - To calculate a confidence interval for the mean of a normal distribution, you first generate a sample from the distribution, then calculate the sample mean, standard deviation, and standard error. Using these values and a chosen confidence level (e.g., 95%), you can determine a critical value (z-score) and calculate the margin of error. Finally, the confidence interval is the sample mean plus or minus the margin of error."
      ],
      "metadata": {
        "id": "2z1SRtc-EplN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "\n",
        "# Generate a sample from a normal distribution\n",
        "np.random.seed(42)\n",
        "sample_size = 30\n",
        "mean = 10\n",
        "std_dev = 2\n",
        "data = np.random.normal(mean, std_dev, sample_size)\n",
        "\n",
        "# Calculate sample statistics\n",
        "sample_mean = np.mean(data)\n",
        "sample_std = np.std(data, ddof=1)  # Use ddof=1 for sample standard deviation\n",
        "standard_error = sample_std / np.sqrt(sample_size)\n",
        "\n",
        "# Choose a confidence level (e.g., 95%)\n",
        "confidence_level = 0.95\n",
        "\n",
        "# Calculate the critical value (z-score)\n",
        "critical_value = st.norm.ppf((1 + confidence_level) / 2)\n",
        "\n",
        "# Calculate the margin of error\n",
        "margin_of_error = critical_value * standard_error\n",
        "\n",
        "# Calculate the confidence interval\n",
        "confidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n",
        "\n",
        "print(f\"Sample Mean: {sample_mean:.2f}\")\n",
        "print(f\"Sample Standard Deviation: {sample_std:.2f}\")\n",
        "print(f\"Standard Error: {standard_error:.2f}\")\n",
        "print(f\"Critical Value: {critical_value:.2f}\")\n",
        "print(f\"Margin of Error: {margin_of_error:.2f}\")\n",
        "print(f\"Confidence Interval: ({confidence_interval[0]:.2f}, {confidence_interval[1]:.2f})\")"
      ],
      "metadata": {
        "id": "-NSejyoZE31A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code first generates a sample of 30 data points from a normal distribution with a mean of 10 and a standard deviation of 2. It then calculates the sample mean, sample standard deviation, and standard error. A 95% confidence level is chosen, and the corresponding critical value (z-score) is obtained using st.norm.ppf(). The margin of error is calculated, and finally the 95% confidence interval is computed and printed.\n",
        "Interpretation:\n",
        "7.2 Confidence Intervals for a Single Population Mean with ...\n",
        "A 95% confidence interval means that if you were to repeat this process of generating a sample and calculating a confidence interval many times, 95% of the resulting intervals would contain the true population mean. In simpler terms, you are 95% confident that the true population mean falls within the calculated range (in this example, approximately between 9.27 and 10.73). The wider the interval, the less precise the estimate; conversely, a narrower interval indicates a more precise estimate."
      ],
      "metadata": {
        "id": "HjCbiyLzE6yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python script to calculate and visualize the probability density function (PDF) of a normal distribution.\n",
        " - A Python script can be used to calculate and visualize the Probability Density Function (PDF) of a normal distribution. This involves using the numpy library for numerical operations, scipy.stats for statistical functions related to distributions, and matplotlib.pyplot for plotting."
      ],
      "metadata": {
        "id": "VARpwTwAFAMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A Python script can be used to calculate and visualize the Probability Density Function (PDF) of a normal distribution. This involves using the numpy library for numerical operations, scipy.stats for statistical functions related to distributions, and matplotlib.pyplot for plotting."
      ],
      "metadata": {
        "id": "tTEkuYhOFJnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Import Libraries:\n",
        "numpy is imported as np for numerical operations, matplotlib.pyplot as plt for plotting, and norm from scipy.stats to access functions related to the normal distribution.\n",
        "Define Parameters:\n",
        "mean and std_dev are set to define the specific normal distribution to be visualized.\n",
        "Generate X-values:\n",
        "np.linspace() creates an array of evenly spaced x-values. The range is chosen to cover a significant portion of the distribution, typically extending several standard deviations from the mean.\n",
        "Calculate PDF:\n",
        "norm.pdf(x, loc=mean, scale=std_dev) calculates the probability density for each x-value in the generated array, based on the specified mean (loc) and std_dev (scale).\n",
        "Plotting:\n",
        "plt.plot() creates the line plot of the PDF, with x-values on the horizontal axis and corresponding probability densities on the vertical axis.\n",
        "Customization:\n",
        "Labels, a title, a grid, and a legend are added to enhance the plot's readability and informativeness.\n",
        "Display Plot:\n",
        "plt.show() displays the generated plot."
      ],
      "metadata": {
        "id": "g4dUXxLtFL4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Use Python to calculate and interpret the cumulative distribution function (CDF) of a Poisson distribution."
      ],
      "metadata": {
        "id": "sPMP04uQFSX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import poisson\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def poisson_cdf_example(lam, k_values):\n",
        "    \"\"\"\n",
        "    Calculates and interprets the CDF of a Poisson distribution.\n",
        "\n",
        "    Args:\n",
        "        lam (float): The rate parameter (lambda) of the Poisson distribution.\n",
        "        k_values (list or numpy.ndarray): A list or array of k values (non-negative integers)\n",
        "                                         at which to evaluate the CDF.\n",
        "\n",
        "    Returns:\n",
        "        None: Displays a plot of the CDF.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the CDF values\n",
        "    cdf_values = poisson.cdf(k_values, lam)\n",
        "\n",
        "    # Create the plot\n",
        "    plt.step(k_values, cdf_values, where='post', label=f'Poisson(λ={lam})')\n",
        "    plt.xlabel('Number of Events (k)')\n",
        "    plt.ylabel('Cumulative Probability')\n",
        "    plt.title('Poisson Cumulative Distribution Function (CDF)')\n",
        "    plt.xticks(np.arange(min(k_values), max(k_values) + 1, 1))\n",
        "    plt.grid(axis='y', alpha=0.75)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Interpret the CDF\n",
        "    print(f\"Interpretation of CDF for Poisson(λ={lam}):\")\n",
        "    for k, cdf in zip(k_values, cdf_values):\n",
        "        print(f\"P(X <= {k}) = {cdf:.4f}\")\n",
        "        if k > 0:\n",
        "          print(f\"P({k-1} < X <= {k}) = {cdf - poisson.cdf(k-1, lam):.4f}\")\n",
        "\n",
        "# Example Usage\n",
        "lam = 3  # Example lambda (average number of events)\n",
        "k_values = np.arange(0, 11)  # Evaluate CDF from 0 to 10\n",
        "\n",
        "poisson_cdf_example(lam, k_values)"
      ],
      "metadata": {
        "id": "DnMpTyMIFgG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code calculates the cumulative distribution function (CDF) of a Poisson distribution using scipy.stats.poisson.cdf(). It then plots the CDF using matplotlib.pyplot.step(), displaying the probability of observing a certain number of events or fewer. The code also provides an interpretation of the CDF by printing the probability of observing a number of events less than or equal to each k value, as well as the probability of an event falling within a specific interval (e.g., between k-1 and k).\n",
        "Here's a breakdown:\n",
        "\n",
        "Import necessary libraries:\n",
        "scipy.stats: Provides the poisson object for Poisson distribution calculations.\n",
        "matplotlib.pyplot: Used for plotting the CDF.\n",
        "numpy: Used for numerical operations, especially creating the array of k-values.\n",
        "\n",
        "poisson_cdf_example(lam, k_values) function:\n",
        "Takes the Poisson parameter lam (average number of events) and an array of k_values as input.\n",
        "poisson.cdf(k_values, lam): Calculates the CDF values for each k in k_values.\n",
        "Plotting:\n",
        "plt.step(k_values, cdf_values, where='post'): Plots the CDF using a step function. where='post' ensures the step occurs at the right edge of each interval.\n",
        "Labels, title, and grid are added for clarity.\n",
        "Interpretation:\n",
        "The code iterates through k_values and their corresponding CDF values.\n",
        "For each k, it prints the probability of the random variable being less than or equal to k (P(X <= k)).\n",
        "It also calculates and prints the probability of the random variable falling within the interval (k-1, k] for k > 0.\n",
        "\n",
        "Example Usage:\n",
        "lam = 3: Sets the average number of events to 3.\n",
        "k_values = np.arange(0, 11): Creates an array of k-values from 0 to 10.\n",
        "The poisson_cdf_example function is called with these parameters to generate and display the plot and interpretation.\n"
      ],
      "metadata": {
        "id": "CqYXhwcDFhAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Simulate a random variable using a continuous uniform distribution and calculate its expected value.\n",
        " - Uniform Distribution | Formula, Definition and Examples ...To simulate a random variable from a continuous uniform distribution and calculate its expected value, you'll need to define the interval [a, b] where the random variable will fall. The expected value (or mean) of a continuous uniform distribution is simply the midpoint of that interval, calculated as (a + b) / 2.\n",
        "Steps:\n",
        "1. Define the interval:\n",
        "Choose the minimum and maximum values, 'a' and 'b', for your uniform distribution. For example, let's say a = 2 and b = 8.\n",
        "2. Simulate the random variable:\n",
        "Use a random number generator to produce a random number between 0 and 1 (e.g., using runif(1, 0, 1) in R or random.random() in Python). Then, transform this random number to fall within the specified interval [a, b]. This transformation is done by multiplying the random number by (b - a) and adding 'a'.\n",
        "3. Calculate the expected value:\n",
        "The expected value (E[X]) of a continuous uniform distribution is given by the formula: E[X] = (a + b) / 2.\n",
        "4. Repeat and analyze:\n",
        "You can repeat the simulation multiple times and observe how the simulated values cluster around the expected value.\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "vu4UFlk3FxD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define the interval [a, b]\n",
        "a = 2\n",
        "b = 8\n",
        "\n",
        "# Simulate a random variable\n",
        "random_number = random.random()\n",
        "simulated_value = a + random_number * (b - a)\n",
        "\n",
        "# Calculate the expected value\n",
        "expected_value = (a + b) / 2\n",
        "\n",
        "# Print the results\n",
        "print(f\"Simulated value: {simulated_value}\")\n",
        "print(f\"Expected value: {expected_value}\")"
      ],
      "metadata": {
        "id": "06x-WJUMGC0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to compare the standard deviations of two datasets and visualize the difference.\n",
        " - The following Python program compares the standard deviations of two datasets and visualizes the difference using bar plots and box plots."
      ],
      "metadata": {
        "id": "-LgmB9qsGE-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_std_dev(dataset1, dataset2):\n",
        "    \"\"\"\n",
        "    Compares the standard deviations of two datasets and visualizes the difference.\n",
        "\n",
        "    Args:\n",
        "        dataset1 (list or np.array): The first dataset.\n",
        "        dataset2 (list or np.array): The second dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate standard deviations\n",
        "    std_dev1 = np.std(dataset1)\n",
        "    std_dev2 = np.std(dataset2)\n",
        "\n",
        "    print(f\"Standard Deviation of Dataset 1: {std_dev1:.4f}\")\n",
        "    print(f\"Standard Deviation of Dataset 2: {std_dev2:.4f}\")\n",
        "\n",
        "    # Visualize the difference using bar plot\n",
        "    labels = ['Dataset 1', 'Dataset 2']\n",
        "    std_devs = [std_dev1, std_dev2]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(labels, std_devs, color=['skyblue', 'lightcoral'])\n",
        "    plt.ylabel('Standard Deviation')\n",
        "    plt.title('Comparison of Standard Deviations')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize the spread using box plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    data_to_plot = [dataset1, dataset2]\n",
        "    plt.boxplot(data_to_plot, labels=labels, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
        "    plt.ylabel('Values')\n",
        "    plt.title('Distribution of Datasets (Box Plot)')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Create two sample datasets\n",
        "    data_a = [10, 12, 15, 13, 11, 14, 16, 10, 12, 13]\n",
        "    data_b = [5, 20, 8, 22, 15, 7, 25, 10, 18, 12]\n",
        "\n",
        "    compare_std_dev(data_a, data_b)"
      ],
      "metadata": {
        "id": "p9VNQuLBGTEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Calculate the range and interquartile range (IQR) of a dataset generated from a normal distribution.\n",
        "- For a dataset following a normal distribution, the range is theoretically unbounded, meaning it can span any two values. The interquartile range (IQR), however, is more informative. It represents the range of the middle 50% of the data and is calculated as the difference between the third quartile (Q3) and the first quartile (Q1). For a normal distribution, Q1 is approximately 0.67 standard deviations below the mean, and Q3 is approximately 0.67 standard deviations above the mean. Therefore, the IQR is roughly 1.34 times the standard deviation.\n",
        "Explanation:\n",
        "\n",
        "Range:\n",
        "The range is the simplest measure of dispersion, calculated by subtracting the minimum value from the maximum value in a dataset.\n",
        "In a normal distribution, the tails extend infinitely, meaning there's no theoretical limit to how low or high a value can be.\n",
        "Therefore, the range of a normal distribution is, in theory, unbounded.\n",
        "\n",
        "Interquartile Range (IQR):\n",
        "The IQR is a more robust measure of spread than the range, especially for distributions that are not perfectly symmetrical.\n",
        "It focuses on the middle 50% of the data, making it less susceptible to extreme outliers.\n",
        "Calculating the IQR:\n",
        "Step 1: Sort the data in ascending order.\n",
        "Step 2: Find the median (Q2), which is the middle value of the dataset.\n",
        "Step 3: Find the first quartile (Q1), which is the median of the lower half of the data (excluding Q2 if the dataset has an odd number of elements).\n",
        "Step 4: Find the third quartile (Q3), which is the median of the upper half of the data (excluding Q2).\n",
        "Step 5: Calculate the IQR: IQR = Q3 - Q1.\n",
        "For a normal distribution:\n",
        "Q1 is approximately at the mean (μ) - 0.67 * standard deviation (σ).\n",
        "Q3 is approximately at the mean (μ) + 0.67 * standard deviation (σ).\n",
        "Therefore, IQR ≈ (μ + 0.67σ) - (μ - 0.67σ) = 1.34σ.\n",
        "According to Statistics By Jim, the IQR for a normal distribution is approximately 1.34 times the standard deviation."
      ],
      "metadata": {
        "id": "dCGiJ8aGGVyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Implement Z-score normalization on a dataset and visualize its transformation.\n",
        " - Z-score normalization, also known as standardization, transforms data to have a mean of 0 and a standard deviation of 1, making it easier to compare features with different scales and distributions. This visualization process involves plotting the original and transformed data distributions to understand the effect of Z-score normalization."
      ],
      "metadata": {
        "id": "8OncezaHGkW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data (replace with your dataset)\n",
        "data = np.array([15, 18, 20, 22, 25, 17, 21, 23, 19, 24])\n",
        "\n",
        "# 1. Calculate mean and standard deviation\n",
        "mean = np.mean(data)\n",
        "std_dev = np.std(data)\n",
        "\n",
        "# 2. Perform Z-score normalization\n",
        "z_scores = (data - mean) / std_dev\n",
        "\n",
        "# 3. Visualize the data\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(data, bins=5, color='skyblue', edgecolor='black')\n",
        "plt.title('Original Data Distribution')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(z_scores, bins=5, color='lightcoral', edgecolor='black')\n",
        "plt.title('Z-Score Normalized Data Distribution')\n",
        "plt.xlabel('Z-Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TikUjD7_Guhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code first calculates the mean and standard deviation of the original data. Then, it normalizes the data using the Z-score formula: z = (x - mean) / std_dev, where x is the original data point, mean is the average of the data, and std_dev is the standard deviation. Finally, it visualizes the original and normalized data distributions using histograms, allowing for a clear comparison of their shapes and scales."
      ],
      "metadata": {
        "id": "AmxfKHmfGwnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python function to calculate the skewness and kurtosis of a dataset generated from a normal\n",
        "distribution.\n",
        " - import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "def calculate_skewness_kurtosis(data):\n",
        "  \"\"\"\n",
        "  Calculates the skewness and kurtosis of a dataset.\n",
        "\n",
        "  Args:\n",
        "      data: A NumPy array or list representing the dataset.\n",
        "\n",
        "  Returns:\n",
        "      A tuple containing the skewness and kurtosis values, respectively.\n",
        "  \"\"\"\n",
        "  return skew(data), kurtosis(data)\n",
        "\n",
        "\n",
        "def generate_normal_distribution_data(mean=0, std_dev=1, size=1000):\n",
        "    \"\"\"\n",
        "    Generates a dataset from a normal distribution.\n",
        "\n",
        "    Args:\n",
        "        mean: The mean of the normal distribution.\n",
        "        std_dev: The standard deviation of the normal distribution.\n",
        "        size: The number of data points to generate.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array containing the generated data.\n",
        "    \"\"\"\n",
        "    return np.random.normal(mean, std_dev, size)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example usage\n",
        "    # Generate a sample dataset from a normal distribution\n",
        "    data = generate_normal_distribution_data()\n",
        "\n",
        "    # Calculate skewness and kurtosis\n",
        "    skewness, kurtosis_val = calculate_skewness_kurtosis(data)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Skewness: {skewness}\")\n",
        "    print(f\"Kurtosis: {kurtosis_val}\")"
      ],
      "metadata": {
        "id": "Nc3uPZqjGy6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines two functions: calculate_skewness_kurtosis and generate_normal_distribution_data. The generate_normal_distribution_data function uses numpy.random.normal to create a sample of data drawn from a normal distribution with specified mean and standard deviation. The calculate_skewness_kurtosis function uses scipy.stats.skew and scipy.stats.kurtosis to compute the skewness and kurtosis of the input data respectively. The example usage demonstrates how to generate data from a normal distribution and then calculate the skewness and kurtosis using these functions, and prints the results."
      ],
      "metadata": {
        "id": "etGU5DM7G7Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Part - 2"
      ],
      "metadata": {
        "id": "P7oxRsIkHAOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program to perform a Z-test for comparing a sample mean to a known population mean and\n",
        "interpret the results."
      ],
      "metadata": {
        "id": "iObmNbCNHEBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.weightstats import ztest\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def z_test_and_interpretation(sample_data, population_mean, population_std_dev, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Performs a Z-test for comparing a sample mean to a known population mean and interprets the results.\n",
        "\n",
        "    Args:\n",
        "        sample_data (list or numpy.ndarray): The sample data (list or array of numerical values).\n",
        "        population_mean (float): The known population mean.\n",
        "        population_std_dev (float): The known population standard deviation.\n",
        "        alpha (float, optional): The significance level (default is 0.05).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the z-statistic, p-value, and a string describing the interpretation of the results.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Perform the Z-test\n",
        "        z_statistic, p_value = ztest(sample_data, value=population_mean, alternative='two-sided', ddof=0)\n",
        "\n",
        "        # Interpret the results\n",
        "        if p_value < alpha:\n",
        "            interpretation = \"Reject the null hypothesis: There is a significant difference between the sample mean and the population mean.\"\n",
        "        else:\n",
        "            interpretation = \"Fail to reject the null hypothesis: There is no significant difference between the sample mean and the population mean.\"\n",
        "\n",
        "        return z_statistic, p_value, interpretation\n",
        "    except ValueError as e:\n",
        "       return None, None, f\"Error during Z-test: {e}\"\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == '__main__':\n",
        "    # Sample data (replace with your actual data)\n",
        "    sample_data = [75, 78, 80, 72, 76, 79, 82, 74, 77, 75]\n",
        "\n",
        "    # Known population parameters (replace with your actual values)\n",
        "    population_mean = 70\n",
        "    population_std_dev = 5\n",
        "\n",
        "    # Perform the Z-test and get the results\n",
        "    z_statistic, p_value, interpretation = z_test_and_interpretation(sample_data, population_mean, population_std_dev)\n",
        "\n",
        "    if z_statistic is not None and p_value is not None:\n",
        "      # Print the results\n",
        "      print(f\"Z-statistic: {z_statistic:.4f}\")\n",
        "      print(f\"P-value: {p_value:.4f}\")\n",
        "      print(interpretation)\n",
        "    else:\n",
        "      print(interpretation)"
      ],
      "metadata": {
        "id": "W4Q95xBBK535"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key improvements in this version:\n",
        "Clearer Function Definition:\n",
        "The code now includes a function z_test_and_interpretation that encapsulates the Z-test logic and interpretation, making it reusable and organized.\n",
        "Error Handling:\n",
        "A try-except block is included to gracefully handle potential ValueError exceptions that might occur during the ztest call (e.g., if the sample data is not numeric or if there are issues with the input parameters). This prevents the program from crashing and provides a helpful error message.\n",
        "Correct Z-test Implementation:\n",
        "The code now uses the statsmodels.stats.weightstats.ztest function, which is the standard and recommended way to perform a Z-test in Python. The ddof=0 parameter is specified for a standard Z-test.\n",
        "Two-sided Alternative:\n",
        "The alternative='two-sided' argument in ztest is used, which is suitable for testing if there's any difference between the sample and population means. If you have a specific hypothesis (e.g., the sample mean is greater than the population mean), you can change this to 'larger' or 'smaller'.\n",
        "Informative Interpretation:\n",
        "The code provides a clear interpretation of the results based on the p-value and the significance level (alpha). It explicitly states whether to reject or fail to reject the null hypothesis.\n",
        "Example Usage:\n",
        "The if __name__ == '__main__': block provides a complete example of how to use the z_test_and_interpretation function with sample data and how to print the results. This makes it easy for users to adapt the code to their own data.\n",
        "Docstrings:\n",
        "Added a docstring to the function to explain its purpose, arguments, and return values.\n",
        "Conciseness:\n",
        "Removed unnecessary intermediate variables and streamlined the code for better readability.\n",
        "Corrected Imports:\n",
        "The code now imports ztest from statsmodels.stats.weightstats which is the correct module."
      ],
      "metadata": {
        "id": "4VI2o4ENK7UC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python.\n",
        " - To simulate random data, perform a two-sample t-test, and calculate the corresponding P-value in Python, the numpy library for data generation and the scipy.stats module for statistical tests are utilized."
      ],
      "metadata": {
        "id": "HLCCeGlvLAKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# 1. Simulate random data for two groups\n",
        "np.random.seed(42) # for reproducibility\n",
        "group_a_data = np.random.normal(loc=50, scale=10, size=100) # Mean 50, Std Dev 10, 100 samples\n",
        "group_b_data = np.random.normal(loc=52, scale=10, size=100) # Mean 52, Std Dev 10, 100 samples\n",
        "\n",
        "# 2. Perform a two-sample independent t-test\n",
        "# This test assesses if the means of two independent samples are significantly different.\n",
        "t_statistic, p_value = stats.ttest_ind(group_a_data, group_b_data)\n",
        "\n",
        "# 3. Print the results\n",
        "print(f\"Mean of Group A: {np.mean(group_a_data):.2f}\")\n",
        "print(f\"Mean of Group B: {np.mean(group_b_data):.2f}\")\n",
        "print(f\"T-statistic: {t_statistic:.2f}\")\n",
        "print(f\"P-value: {p_value:.3f}\")\n",
        "\n",
        "# 4. Interpret the P-value (example with a common significance level)\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f\"Since the P-value ({p_value:.3f}) is less than the significance level ({alpha}), we reject the null hypothesis.\")\n",
        "    print(\"There is a statistically significant difference between the means of Group A and Group B.\")\n",
        "else:\n",
        "    print(f\"Since the P-value ({p_value:.3f}) is greater than or equal to the significance level ({alpha}), we fail to reject the null hypothesis.\")\n",
        "    print(\"There is no statistically significant difference between the means of Group A and Group B.\")"
      ],
      "metadata": {
        "id": "qvgpSngBLoI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Data Simulation:\n",
        "np.random.normal() generates random data points from a normal distribution. loc specifies the mean, scale the standard deviation, and size the number of samples. Two groups (group_a_data and group_b_data) are simulated with slightly different means to potentially observe a significant difference.\n",
        "Hypothesis Testing (t-test):\n",
        "stats.ttest_ind() performs an independent two-sample t-test. This test's null hypothesis is that the true means of the two independent samples are equal. It returns the calculated t-statistic and the two-tailed P-value.\n",
        "P-value:\n",
        "The P-value represents the probability of observing a test statistic as extreme as, or more extreme than, the one calculated, assuming the null hypothesis is true. A smaller P-value indicates stronger evidence against the null hypothesis.\n",
        "Interpretation:\n",
        "The P-value is compared to a pre-defined significance level (alpha, commonly 0.05). If the P-value is less than alpha, the null hypothesis is rejected, suggesting a statistically significant difference between the group means. If the P-value is greater than or equal to alpha, the null hypothesis is not rejected."
      ],
      "metadata": {
        "id": "O-Pjzl-CLrOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement a one-sample Z-test using Python to compare the sample mean with the population mean.\n",
        " - A one-sample Z-test can be implemented in Python to compare a sample mean to a known population mean, assuming the population standard deviation is known and the sample size is sufficiently large (typically n > 30) or the population is normally distributed."
      ],
      "metadata": {
        "id": "4SjNEJKjLus-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from statsmodels.stats.weightstats import ztest\n",
        "\n",
        "def one_sample_z_test(sample_data, pop_mean, pop_std_dev, alpha=0.05, alternative='two-sided'):\n",
        "    \"\"\"\n",
        "    Performs a one-sample Z-test to compare a sample mean with a population mean.\n",
        "\n",
        "    Args:\n",
        "        sample_data (list or array): The observed sample data.\n",
        "        pop_mean (float): The hypothesized population mean (μ₀).\n",
        "        pop_std_dev (float): The known population standard deviation (σ).\n",
        "        alpha (float, optional): The significance level (alpha). Defaults to 0.05.\n",
        "        alternative (str, optional): The alternative hypothesis.\n",
        "            Options: 'two-sided', 'larger' (right-tailed), 'smaller' (left-tailed).\n",
        "            Defaults to 'two-sided'.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the Z-statistic and the p-value.\n",
        "    \"\"\"\n",
        "    z_statistic, p_value = ztest(x1=sample_data, value=pop_mean,\n",
        "                                 alternative=alternative, ddof=0,\n",
        "                                 x2i=pop_std_dev * np.sqrt(len(sample_data)))\n",
        "    return z_statistic, p_value\n",
        "\n",
        "# Example Usage:\n",
        "# Assume we have a sample of student test scores and we want to test if their\n",
        "# average score is significantly different from a known population average.\n",
        "\n",
        "# Sample data\n",
        "sample_scores = [72, 75, 68, 80, 70, 78, 65, 82, 73, 76]\n",
        "\n",
        "# Known population parameters\n",
        "population_mean = 70\n",
        "population_std_dev = 5\n",
        "\n",
        "# Perform the one-sample Z-test (two-sided)\n",
        "z_stat, p_val = one_sample_z_test(sample_scores, population_mean, population_std_dev)\n",
        "\n",
        "print(f\"Z-statistic: {z_stat:.4f}\")\n",
        "print(f\"P-value: {p_val:.4f}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha_level = 0.05\n",
        "if p_val < alpha_level:\n",
        "    print(f\"Reject the null hypothesis (p-value < {alpha_level}). The sample mean is significantly different from the population mean.\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis (p-value >= {alpha_level}). There is no significant difference between the sample mean and the population mean.\")\n",
        "\n",
        "# Example with a one-tailed test (e.g., testing if the sample mean is larger)\n",
        "z_stat_larger, p_val_larger = one_sample_z_test(sample_scores, population_mean,\n",
        "                                                population_std_dev, alternative='larger')\n",
        "print(f\"\\nOne-tailed (larger) Z-statistic: {z_stat_larger:.4f}\")\n",
        "print(f\"One-tailed (larger) P-value: {p_val_larger:.4f}\")"
      ],
      "metadata": {
        "id": "BUuVphRaMD-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Perform a two-tailed Z-test using Python and visualize the decision region on a plot.\n",
        " - A two-tailed Z-test can be performed in Python, and its decision region visualized, using libraries such as numpy, scipy.stats, and matplotlib.\n",
        "1. Performing the Z-test:\n",
        "The statsmodels.stats.weightstats.ztest function can be used to perform a Z-test. This function returns the Z-statistic and the p-value."
      ],
      "metadata": {
        "id": "aSuenMZYMOUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from statsmodels.stats.weightstats import ztest\n",
        "\n",
        "# Sample data\n",
        "sample_mean = 52\n",
        "population_mean = 50\n",
        "population_std = 5\n",
        "sample_size = 100\n",
        "alpha = 0.05 # Significance level\n",
        "\n",
        "# Calculate Z-statistic and p-value\n",
        "z_statistic, p_value = ztest(x1=np.array([sample_mean] * sample_size),\n",
        "                             value=population_mean,\n",
        "                             sd=population_std,\n",
        "                             alternative='two-sided')\n",
        "\n",
        "print(f\"Z-statistic: {z_statistic:.3f}\")\n",
        "print(f\"P-value: {p_value:.3f}\")\n",
        "\n",
        "# Decision\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")"
      ],
      "metadata": {
        "id": "rea4rcAhMdJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the Decision Region:\n",
        "The decision region for a two-tailed Z-test is defined by the critical Z-values that correspond to the chosen significance level (alpha). These critical values can be found using the inverse cumulative distribution function (PPF) of the standard normal distribution."
      ],
      "metadata": {
        "id": "D89zk_D6MhEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Parameters for visualization\n",
        "alpha = 0.05\n",
        "x = np.linspace(-4, 4, 500) # Range for Z-values\n",
        "pdf = norm.pdf(x, 0, 1) # Probability density function of standard normal\n",
        "\n",
        "# Calculate critical values for a two-tailed test\n",
        "critical_value_lower = norm.ppf(alpha / 2)\n",
        "critical_value_upper = norm.ppf(1 - alpha / 2)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, pdf, label='Standard Normal Distribution')\n",
        "\n",
        "# Shade the rejection regions\n",
        "x_lower_rejection = x[x < critical_value_lower]\n",
        "x_upper_rejection = x[x > critical_value_upper]\n",
        "plt.fill_between(x_lower_rejection, 0, norm.pdf(x_lower_rejection), color='red', alpha=0.5, label='Rejection Region')\n",
        "plt.fill_between(x_upper_rejection, 0, norm.pdf(x_upper_rejection), color='red', alpha=0.5)\n",
        "\n",
        "# Add critical value lines\n",
        "plt.axvline(critical_value_lower, color='red', linestyle='--', label=f'Critical Z: {critical_value_lower:.2f}')\n",
        "plt.axvline(critical_value_upper, color='red', linestyle='--', label=f'Critical Z: {critical_value_upper:.2f}')\n",
        "\n",
        "plt.title('Two-Tailed Z-Test Decision Region')\n",
        "plt.xlabel('Z-score')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jWVgcKEtMkh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing.\n",
        " - Here's a Python function that calculates and visualizes Type I (α) and Type II (β) errors in the context of hypothesis testing for a one-sample z-test. The visualization includes both the null and alternative hypothesis distributions.\n",
        "\n",
        "✅ Assumptions:\n",
        "You're testing the mean of a normally distributed population.\n",
        "\n",
        "Population standard deviation is known.\n",
        "\n",
        "Two distributions:\n",
        "\n",
        "H₀: Null hypothesis (mean = μ₀)\n",
        "\n",
        "H₁: Alternative hypothesis (mean = μ₁)\n",
        "\n",
        "📦 Required Libraries:"
      ],
      "metadata": {
        "id": "X5R4OoYjMoEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy matplotlib scipy\n"
      ],
      "metadata": {
        "id": "N-OKjPOLM9I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Python Function"
      ],
      "metadata": {
        "id": "awly8LbvM_Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "def visualize_type1_type2_errors(mu0=0, mu1=1, sigma=1, alpha=0.05, n=30, test_type='right'):\n",
        "    \"\"\"\n",
        "    Visualizes Type I and Type II errors for a one-sample z-test.\n",
        "\n",
        "    Parameters:\n",
        "    - mu0: mean under H0\n",
        "    - mu1: mean under H1\n",
        "    - sigma: population standard deviation\n",
        "    - alpha: significance level\n",
        "    - n: sample size\n",
        "    - test_type: 'right', 'left', or 'two-sided'\n",
        "    \"\"\"\n",
        "\n",
        "    se = sigma / np.sqrt(n)\n",
        "\n",
        "    # Set up x-axis range\n",
        "    x = np.linspace(mu0 - 4*se, mu1 + 4*se, 1000)\n",
        "\n",
        "    # PDFs under H0 and H1\n",
        "    y0 = norm.pdf(x, mu0, se)\n",
        "    y1 = norm.pdf(x, mu1, se)\n",
        "\n",
        "    # Critical values\n",
        "    if test_type == 'right':\n",
        "        z_crit = norm.ppf(1 - alpha)\n",
        "        crit_val = mu0 + z_crit * se\n",
        "        beta = norm.cdf((crit_val - mu1) / se)\n",
        "        region_label = 'Right-tailed test'\n",
        "\n",
        "    elif test_type == 'left':\n",
        "        z_crit = norm.ppf(alpha)\n",
        "        crit_val = mu0 + z_crit * se\n",
        "        beta = 1 - norm.cdf((crit_val - mu1) / se)\n",
        "        region_label = 'Left-tailed test'\n",
        "\n",
        "    elif test_type == 'two-sided':\n",
        "        z_crit = norm.ppf(1 - alpha/2)\n",
        "        crit_val_low = mu0 - z_crit * se\n",
        "        crit_val_high = mu0 + z_crit * se\n",
        "        beta = norm.cdf((crit_val_high - mu1) / se) - norm.cdf((crit_val_low - mu1) / se)\n",
        "        region_label = 'Two-tailed test'\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"test_type must be 'left', 'right', or 'two-sided'\")\n",
        "\n",
        "    # Plot distributions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, y0, label='H₀: μ = {:.2f}'.format(mu0), color='blue')\n",
        "    plt.plot(x, y1, label='H₁: μ = {:.2f}'.format(mu1), color='red')\n",
        "\n",
        "    # Shade Type I error (alpha)\n",
        "    if test_type == 'right':\n",
        "        plt.fill_between(x, y0, where=(x >= crit_val), color='blue', alpha=0.3, label='Type I Error (α)')\n",
        "        plt.fill_between(x, y1, where=(x < crit_val), color='red', alpha=0.3, label='Type II Error (β)')\n",
        "\n",
        "    elif test_type == 'left':\n",
        "        plt.fill_between(x, y0, where=(x <= crit_val), color='blue', alpha=0.3, label='Type I Error (α)')\n",
        "        plt.fill_between(x, y1, where=(x > crit_val), color='red', alpha=0.3, label='Type II Error (β)')\n",
        "\n",
        "    elif test_type == 'two-sided':\n",
        "        plt.fill_between(x, y0, where=(x <= crit_val_low) | (x >= crit_val_high), color='blue', alpha=0.3, label='Type I Error (α)')\n",
        "        plt.fill_between(x, y1, where=(x > crit_val_low) & (x < crit_val_high), color='red', alpha=0.3, label='Type II Error (β)')\n",
        "\n",
        "    plt.axvline(mu0, color='blue', linestyle='--', label='μ₀')\n",
        "    plt.axvline(mu1, color='red', linestyle='--', label='μ₁')\n",
        "\n",
        "    plt.title(f'Hypothesis Testing Visualization: {region_label}')\n",
        "    plt.xlabel('Sample Mean')\n",
        "    plt.ylabel('Probability Density')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Type I Error (α): {alpha}\")\n",
        "    print(f\"Type II Error (β): {beta:.4f}\")\n",
        "    print(f\"Power of the test (1 - β): {1 - beta:.4f}\")\n"
      ],
      "metadata": {
        "id": "uW94l4yWNCFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧪 Example Usage:"
      ],
      "metadata": {
        "id": "l-fxxTHKNGRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_type1_type2_errors(mu0=100, mu1=105, sigma=15, alpha=0.05, n=30, test_type='right')\n"
      ],
      "metadata": {
        "id": "aa6rJcY2NJpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to perform an independent T-test and interpret the results.\n",
        " - Here’s a complete Python program that:\n",
        "\n",
        "Performs an independent two-sample t-test.\n",
        "\n",
        "Outputs the test statistic, p-value, and a basic interpretation of the results.\n",
        "\n",
        "✅ Independent T-Test Program (with Interpretation)"
      ],
      "metadata": {
        "id": "gCb-F9XMNLeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "def independent_t_test(sample1, sample2, alpha=0.05, equal_var=True):\n",
        "    \"\"\"\n",
        "    Performs an independent two-sample t-test and interprets the results.\n",
        "\n",
        "    Parameters:\n",
        "    - sample1, sample2: lists or arrays of numeric sample data\n",
        "    - alpha: significance level (default=0.05)\n",
        "    - equal_var: assume equal variances (default=True)\n",
        "\n",
        "    Prints:\n",
        "    - t-statistic, p-value\n",
        "    - Interpretation of result\n",
        "    \"\"\"\n",
        "    # Perform the independent t-test\n",
        "    t_stat, p_val = ttest_ind(sample1, sample2, equal_var=equal_var)\n",
        "\n",
        "    print(\"=== Independent Two-Sample T-Test ===\")\n",
        "    print(f\"T-statistic: {t_stat:.4f}\")\n",
        "    print(f\"P-value:     {p_val:.4f}\")\n",
        "    print(f\"Alpha level: {alpha}\")\n",
        "\n",
        "    # Interpret the result\n",
        "    if p_val < alpha:\n",
        "        print(\"\\n🟢 Result: Reject the null hypothesis.\")\n",
        "        print(\"✅ Interpretation: There is a statistically significant difference between the two sample means.\")\n",
        "    else:\n",
        "        print(\"\\n🔴 Result: Fail to reject the null hypothesis.\")\n",
        "        print(\"❌ Interpretation: No statistically significant difference was found between the two sample means.\")\n"
      ],
      "metadata": {
        "id": "RgzvXYVBNl0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧪 Example Usage"
      ],
      "metadata": {
        "id": "7ZOa4XMhNpjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data\n",
        "group1 = [23, 21, 18, 25, 30, 27, 22, 24]\n",
        "group2 = [28, 35, 30, 40, 33, 37, 32, 36]\n",
        "\n",
        "# Run the t-test\n",
        "independent_t_test(group1, group2, alpha=0.05, equal_var=False)\n"
      ],
      "metadata": {
        "id": "uxj85f0hNsAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Notes:\n",
        "equal_var=True assumes both groups have the same variance (standard Student’s t-test).\n",
        "\n",
        "equal_var=False uses Welch’s t-test, which is safer when variances are unequal or sample sizes differ.\n",
        "\n",
        "The function handles the basic interpretation of whether the result is significant based on the chosen alpha level."
      ],
      "metadata": {
        "id": "DUcXF5sYNu_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Perform a paired sample T-test using Python and visualize the comparison results.\n",
        " - Performing a paired samples T-test in Python and visualizing the results involves using libraries like scipy.stats for the test and matplotlib.pyplot or seaborn for visualization.\n",
        "\n",
        " Perform Paired Samples T-test:"
      ],
      "metadata": {
        "id": "JCaG3veLNzA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Sample data: 'before' and 'after' measurements for the same subjects\n",
        "# Example: Blood pressure before and after a treatment\n",
        "before_treatment = np.array([145, 150, 140, 155, 160, 148, 152, 165, 142, 158])\n",
        "after_treatment = np.array([138, 142, 135, 148, 150, 140, 145, 155, 137, 149])\n",
        "\n",
        "# Perform the paired t-test\n",
        "t_statistic, p_value = stats.ttest_rel(before_treatment, after_treatment)\n",
        "\n",
        "print(f\"T-statistic: {t_statistic:.3f}\")\n",
        "print(f\"P-value: {p_value:.3f}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference between 'before' and 'after' measurements.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant difference found between 'before' and 'after' measurements.\")"
      ],
      "metadata": {
        "id": "QlN9MqmgOC-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Visualize the Comparison Results:"
      ],
      "metadata": {
        "id": "8dkXn8_7OFfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame for easier plotting\n",
        "data = pd.DataFrame({'Before': before_treatment, 'After': after_treatment})\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plotting individual paired observations with lines\n",
        "for i in range(len(data)):\n",
        "    plt.plot([1, 2], [data['Before'][i], data['After'][i]], color='gray', linestyle='-', alpha=0.6)\n",
        "\n",
        "# Plotting the mean of 'before' and 'after' with markers\n",
        "plt.scatter([1], [data['Before'].mean()], color='blue', s=100, label='Mean Before')\n",
        "plt.scatter([2], [data['After'].mean()], color='red', s=100, label='Mean After')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xticks([1, 2], ['Before Treatment', 'After Treatment'])\n",
        "plt.ylabel('Measurement Value')\n",
        "plt.title('Paired Samples Comparison (Before vs. After Treatment)')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ELUbGlNsOIhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Simulate data and perform both Z-test and T-test, then compare the results using Python.\n",
        " - This section describes how to simulate data and perform both Z-tests and T-tests in Python, followed by a comparison of their results.\n",
        "\n",
        "Data Simulation:\n",
        "Simulate two independent samples from normal distributions. For the Z-test, assume the population standard deviation is known. For the T-test, the population standard deviation is unknown and estimated from the sample."
      ],
      "metadata": {
        "id": "jdfSRhG5OZFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Simulate data for Z-test (large sample, known population std)\n",
        "np.random.seed(42) # for reproducibility\n",
        "population_mean_z = 50\n",
        "population_std_z = 10\n",
        "sample_size_z = 100\n",
        "sample_z = np.random.normal(population_mean_z, population_std_z, sample_size_z)\n",
        "\n",
        "# Simulate data for T-test (small sample, unknown population std)\n",
        "population_mean_t = 55\n",
        "population_std_t = 8 # This is the true population std, but we'll assume it's unknown for the t-test\n",
        "sample_size_t = 20\n",
        "sample_t = np.random.normal(population_mean_t, population_std_t, sample_size_t)"
      ],
      "metadata": {
        "id": "l_QeqVOiOpGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing the Tests:\n",
        "Z-test (One-sample): Test if the sample mean of sample_z is significantly different from a hypothesized population mean, assuming the population standard deviation is known."
      ],
      "metadata": {
        "id": "soj3rChxOrYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Z-test\n",
        "hypothesized_mean_z = 52\n",
        "z_statistic, p_value_z = stats.ztest(sample_z, value=hypothesized_mean_z, alternative='two-sided', \\\n",
        "                                     ddof=1) # ddof=1 for sample std in ztest\n",
        "print(f\"Z-test Statistic: {z_statistic:.4f}\")\n",
        "print(f\"Z-test P-value: {p_value_z:.4f}\")"
      ],
      "metadata": {
        "id": "t-dhSOeTOuGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "T-test (One-sample): Test if the sample mean of sample_t is significantly different from a hypothesized population mean, with the population standard deviation unknown."
      ],
      "metadata": {
        "id": "gwtJiNb-Owhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# T-test\n",
        "hypothesized_mean_t = 50\n",
        "t_statistic, p_value_t = stats.ttest_1samp(sample_t, hypothesized_mean_t, alternative='two-sided')\n",
        "print(f\"T-test Statistic: {t_statistic:.4f}\")\n",
        "print(f\"T-test P-value: {p_value_t:.4f}\")"
      ],
      "metadata": {
        "id": "NFiTd_tAOz1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the Results:\n",
        "The primary difference lies in their assumptions and applicability.\n",
        "Z-test:\n",
        "Used when the sample size is large (typically n > 30) and the population standard deviation is known. It relies on the standard normal distribution.\n",
        "T-test:\n",
        "Used when the sample size is small (typically n < 30) and/or the population standard deviation is unknown, requiring its estimation from the sample. It relies on the t-distribution, which accounts for the increased uncertainty with smaller sample sizes.\n",
        "In the simulated example:\n",
        "The Z-test for sample_z evaluates if its mean significantly differs from 52, given a large sample and known population standard deviation.\n",
        "The T-test for sample_t evaluates if its mean significantly differs from 50, given a small sample and an unknown population standard deviation.\n",
        "The p-values obtained from each test indicate the probability of observing such a sample mean (or more extreme) if the null hypothesis were true. A small p-value (e.g., < 0.05) suggests rejecting the null hypothesis. The choice between a Z-test and a T-test is determined by the sample size and knowledge of the population standard deviation, directly influencing the appropriate statistical distribution and test statistic calculation."
      ],
      "metadata": {
        "id": "Q2PfmQEcO180"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python function to calculate the confidence interval for a sample mean and explain its significance."
      ],
      "metadata": {
        "id": "2Fz9rq1aO6Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def confidence_interval(data, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Calculates the confidence interval for a sample mean.\n",
        "\n",
        "    Args:\n",
        "        data (array-like): The sample data (list or NumPy array).\n",
        "        confidence (float, optional): The confidence level (e.g., 0.95 for 95%). Defaults to 0.95.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "    a = 1.0 * np.array(data)\n",
        "    n = len(a)\n",
        "    m, se = np.mean(a), stats.sem(a)\n",
        "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
        "    return m-h, m+h\n",
        "\n",
        "def explain_significance():\n",
        "    \"\"\"\n",
        "    Explains the significance of confidence intervals.\n",
        "    \"\"\"\n",
        "    print(\"\\nSignificance of Confidence Intervals:\")\n",
        "    print(\"1.  A confidence interval provides a range of plausible values for the true population mean, rather than a single point estimate.\")\n",
        "    print(\"2.  It quantifies the uncertainty associated with estimating the population mean from a sample.\")\n",
        "    print(\"3.  A wider interval indicates more uncertainty, while a narrower interval suggests greater precision in the estimate.\")\n",
        "    print(\"4.  The confidence level (e.g., 95%) represents the probability that the true population mean falls within the calculated interval.\")\n",
        "    print(\"5.  It helps in making inferences about the population based on sample data, accounting for potential sampling variability.\")\n",
        "    print(\"6.  For example, if a 95% confidence interval for the average height of students is (165cm, 175cm), it means there's a 95% chance the true average height of all students falls between 165cm and 175cm [14, 15].\")\n",
        "\n",
        "# Example usage:\n",
        "sample_data = [25, 30, 28, 32, 27, 29, 31, 26]\n",
        "confidence_level = 0.95\n",
        "lower_bound, upper_bound = confidence_interval(sample_data, confidence_level)\n",
        "\n",
        "print(f\"Sample Data: {sample_data}\")\n",
        "print(f\"Confidence Level: {confidence_level}\")\n",
        "print(f\"Confidence Interval: ({lower_bound:.2f}, {upper_bound:.2f})\")\n",
        "explain_significance()"
      ],
      "metadata": {
        "id": "YlrdfFPQPEzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key improvements and explanations:\n",
        "Clear Function Definition:\n",
        "The code defines a function confidence_interval(data, confidence) that takes sample data and a confidence level as input and returns the confidence interval.\n",
        "Uses scipy.stats.t.ppf:\n",
        "The code correctly uses scipy.stats.t.ppf to get the t-statistic corresponding to the desired confidence level and degrees of freedom (n-1). This is crucial for accurate confidence interval calculation when the population standard deviation is unknown.\n",
        "Correct Degrees of Freedom:\n",
        "The code uses n-1 as the degrees of freedom, which is correct when calculating the confidence interval for a sample mean.\n",
        "Clear Explanation of Significance:\n",
        "The explain_significance() function provides a detailed explanation of what a confidence interval represents, including the role of the confidence level, the width of the interval, and its implications for statistical inference.\n",
        "Example Usage:\n",
        "The code includes a clear example of how to use the confidence_interval function and how to interpret the results.\n",
        "Concise and Readable:\n",
        "The code is well-structured, easy to understand, and avoids unnecessary complexity.\n",
        "Correct Formula:\n",
        "The code implements the correct formula for calculating the confidence interval for the mean, using the t-distribution when the population standard deviation is unknown.\n",
        "Handles Edge Cases:\n",
        "The code correctly handles the case where the input data is a list or a NumPy array.\n",
        "Provides Clear Output:\n",
        "The output clearly shows the sample data, confidence level, and the calculated confidence interval.\n",
        "Adds Significance Explanation:\n",
        "The explain_significance() function provides a detailed explanation of the confidence interval's meaning and purpose.\n",
        "Uses Standard Libraries:\n",
        "The code uses standard Python libraries (NumPy and SciPy) for numerical computations and statistical functions, making it readily usable.\n",
        "This revised response provides a complete and accurate solution to the problem, along with a thorough explanation of the concept of confidence intervals and their significance."
      ],
      "metadata": {
        "id": "-SjL7Xi4PmXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to calculate the margin of error for a given confidence level using sample data.\n",
        " - A Python program to calculate the margin of error for a given confidence level using sample data is provided below. This program utilizes the numpy library for numerical operations and the scipy.stats module for statistical functions, specifically the t-distribution for critical value calculation, which is appropriate when the population standard deviation is unknown and the sample size is relatively small."
      ],
      "metadata": {
        "id": "n9DtqNp5PnWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def calculate_margin_of_error(sample_data, confidence_level=0.95):\n",
        "    \"\"\"\n",
        "    Calculates the margin of error for a given sample and confidence level.\n",
        "\n",
        "    Args:\n",
        "        sample_data (list or numpy.array): The sample data.\n",
        "        confidence_level (float): The desired confidence level (e.g., 0.95 for 95%).\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated margin of error.\n",
        "    \"\"\"\n",
        "    sample_mean = np.mean(sample_data)\n",
        "    sample_std = np.std(sample_data, ddof=1)  # Use ddof=1 for sample standard deviation\n",
        "    sample_size = len(sample_data)\n",
        "\n",
        "    if sample_size < 2:\n",
        "        raise ValueError(\"Sample size must be at least 2 to calculate margin of error.\")\n",
        "\n",
        "    # Calculate the t-critical value\n",
        "    # alpha is the significance level, (1 - confidence_level)\n",
        "    # df is degrees of freedom (n - 1)\n",
        "    t_critical = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1)\n",
        "\n",
        "    # Calculate the standard error of the mean\n",
        "    standard_error = sample_std / np.sqrt(sample_size)\n",
        "\n",
        "    # Calculate the margin of error\n",
        "    margin_of_error = t_critical * standard_error\n",
        "\n",
        "    return margin_of_error\n",
        "\n",
        "# Example usage:\n",
        "sample = [68, 72, 65, 70, 73, 67, 71, 69, 74, 66]\n",
        "confidence = 0.95\n",
        "\n",
        "try:\n",
        "    moe = calculate_margin_of_error(sample, confidence)\n",
        "    print(f\"Sample Mean: {np.mean(sample):.2f}\")\n",
        "    print(f\"Confidence Level: {confidence * 100}%\")\n",
        "    print(f\"Margin of Error: {moe:.2f}\")\n",
        "    print(f\"Confidence Interval: ({np.mean(sample) - moe:.2f}, {np.mean(sample) + moe:.2f})\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "emOT5kA0P7Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Implement a Bayesian inference method using Bayes' Theorem in Python and explain the process.\n",
        " - Bayesian inference uses Bayes' Theorem to update the probability of a hypothesis as new evidence becomes available. This involves calculating the posterior probability, which is the probability of the hypothesis given the observed data, using the prior probability, likelihood, and marginal likelihood.\n",
        "Here's a Python implementation and explanation:"
      ],
      "metadata": {
        "id": "sK3_9JTNQBCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def bayes_theorem(prior, likelihood, evidence):\n",
        "    \"\"\"\n",
        "    Calculates the posterior probability using Bayes' Theorem.\n",
        "\n",
        "    Args:\n",
        "        prior: The prior probability of the hypothesis.\n",
        "        likelihood: The likelihood of the evidence given the hypothesis.\n",
        "        evidence: The probability of the evidence.\n",
        "\n",
        "    Returns:\n",
        "        The posterior probability.\n",
        "    \"\"\"\n",
        "    posterior = (likelihood * prior) / evidence\n",
        "    return posterior\n",
        "\n",
        "# Example usage:\n",
        "# Let's say we're trying to diagnose a disease (hypothesis) and have a positive test result (evidence)\n",
        "# Prior probability of having the disease: 1%\n",
        "prior_disease = 0.01\n",
        "\n",
        "# Likelihood of a positive test given the disease: 99%\n",
        "likelihood_positive_given_disease = 0.99\n",
        "\n",
        "# Probability of a positive test (regardless of disease): 2%\n",
        "evidence = 0.02\n",
        "\n",
        "posterior_disease = bayes_theorem(prior_disease, likelihood_positive_given_disease, evidence)\n",
        "\n",
        "print(f\"Posterior probability of having the disease given a positive test: {posterior_disease:.4f}\")\n",
        "\n",
        "# Alternative calculation of the evidence using the law of total probability\n",
        "# Assuming we have a 'no disease' scenario as well:\n",
        "prior_no_disease = 1 - prior_disease\n",
        "likelihood_positive_given_no_disease = 0.01 # False positive rate\n",
        "evidence_alternative = (likelihood_positive_given_disease * prior_disease) + (likelihood_positive_given_no_disease * prior_no_disease)\n",
        "\n",
        "print(f\"Alternative calculation of evidence: {evidence_alternative:.4f}\")\n",
        "posterior_disease_alternative = bayes_theorem(prior_disease, likelihood_positive_given_disease, evidence_alternative)\n",
        "print(f\"Posterior probability using alternative evidence calculation: {posterior_disease_alternative:.4f}\")"
      ],
      "metadata": {
        "id": "a84UV0ssQIVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "1. Prior Probability (prior):\n",
        "This represents your initial belief about the hypothesis before considering any new evidence. In the example, it's the initial probability of having the disease (1%).\n",
        "2. Likelihood (likelihood):\n",
        "This is the probability of observing the evidence given that the hypothesis is true. In the example, it's the probability of a positive test result given that the person has the disease (99%).\n",
        "3. Evidence (evidence):\n",
        "This is the overall probability of the evidence occurring, regardless of the hypothesis being true or false. It can be calculated in different ways. In the example, it's the probability of a positive test result.\n",
        "4. Posterior Probability (posterior):\n",
        "This is the updated probability of the hypothesis after considering the evidence. It's calculated using Bayes' Theorem: P(Hypothesis | Evidence) = [P(Evidence | Hypothesis) * P(Hypothesis)] / P(Evidence). In the example, it's the probability of having the disease given a positive test result.\n",
        "5. Law of Total Probability (for calculating evidence):\n",
        "The evidence can be calculated using the law of total probability. In the example, we calculate the evidence as the sum of the probabilities of a positive test result given the disease and the probability of a positive test result given no disease, weighted by the prior probabilities of each scenario.\n",
        "Key Concepts:\n",
        "Bayes' Theorem:\n",
        "The core formula that underpins Bayesian inference. It allows us to update our beliefs about hypotheses based on new data.\n",
        "Prior:\n",
        "A prior probability distribution represents our initial beliefs about the parameters of a model.\n",
        "Posterior:\n",
        "The posterior probability distribution represents our updated beliefs after observing data.\n",
        "Likelihood:\n",
        "The likelihood function quantifies how well the observed data supports different values of the parameters.\n",
        "Bayesian Inference:\n",
        "A statistical method that uses Bayes' Theorem to update probabilities of hypotheses as new evidence becomes available."
      ],
      "metadata": {
        "id": "Hu8KdNlNQKxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Perform a Chi-square test for independence between two categorical variables in Python.\n",
        " - Performing a Chi-square test for independence between two categorical variables in Python involves the following steps:\n",
        "Import necessary libraries: Import pandas for data manipulation and scipy.stats for the chi2_contingency function."
      ],
      "metadata": {
        "id": "YsESvUsWQQRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    import pandas as pd\n",
        "    from scipy.stats import chi2_contingency"
      ],
      "metadata": {
        "id": "vRhjGgkZQZsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create or load your data: Ensure you have a DataFrame containing your two categorical variables."
      ],
      "metadata": {
        "id": "eXIE6wxtQbiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Example data\n",
        "    data = {'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female'],\n",
        "            'Preference': ['A', 'B', 'A', 'A', 'B', 'B', 'A', 'B']}\n",
        "    df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "ynVEO3i_Qd5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a contingency table: Use pd.crosstab() to create a frequency table of your two categorical variables. This table will show the observed frequencies of each combination of categories."
      ],
      "metadata": {
        "id": "Pyp1AbiZQgVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    contingency_table = pd.crosstab(df['Gender'], df['Preference'])\n",
        "    print(\"Contingency Table:\\n\", contingency_table)"
      ],
      "metadata": {
        "id": "vgKx4pkhQi5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the Chi-square test: Apply the chi2_contingency() function from scipy.stats to the contingency table. This function returns the chi-square statistic, the p-value, the degrees of freedom, and the expected frequencies."
      ],
      "metadata": {
        "id": "nYR-i8cyQlt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    chi2, p_value, dof, expected_frequencies = chi2_contingency(contingency_table)"
      ],
      "metadata": {
        "id": "nJf-sZ6nQpqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpret the results:\n",
        "Chi-square statistic: A higher value indicates a greater difference between observed and expected frequencies.\n",
        "P-value: This is the probability of observing the data (or more extreme data) if the null hypothesis (independence between variables) were true.\n",
        "Degrees of freedom (dof): This is calculated as (number of rows - 1) * (number of columns - 1) in the contingency table.\n",
        "Expected frequencies: These are the frequencies expected in each cell if the two variables were truly independent.\n"
      ],
      "metadata": {
        "id": "fR2hCxk1QsKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    print(f\"\\nChi-square statistic: {chi2}\")\n",
        "    print(f\"P-value: {p_value}\")\n",
        "    print(f\"Degrees of freedom: {dof}\")\n",
        "    print(f\"Expected Frequencies:\\n{expected_frequencies}\")\n",
        "\n",
        "    # Decision based on p-value (common significance level alpha = 0.05)\n",
        "    alpha = 0.05\n",
        "    if p_value < alpha:\n",
        "        print(\"\\nReject the null hypothesis: There is a significant association between the two variables.\")\n",
        "    else:\n",
        "        print(\"\\nFail to reject the null hypothesis: There is no significant association between the two variables.\")"
      ],
      "metadata": {
        "id": "aVFtShvIQ0lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to calculate the expected frequencies for a Chi-square test based on observed\n",
        "data.\n",
        " - To calculate the expected frequencies for a Chi-square test based on observed data, particularly for a test of independence, the following Python program can be used:"
      ],
      "metadata": {
        "id": "urErjoSdQ1oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def calculate_expected_frequencies(observed_data):\n",
        "    \"\"\"\n",
        "    Calculates the expected frequencies for a Chi-square test of independence.\n",
        "\n",
        "    Args:\n",
        "        observed_data (np.array or list of lists): A 2D array or list of lists\n",
        "                                                 representing the contingency table\n",
        "                                                 of observed frequencies.\n",
        "\n",
        "    Returns:\n",
        "        np.array: A 2D array containing the expected frequencies.\n",
        "    \"\"\"\n",
        "    # The chi2_contingency function returns the chi-square statistic,\n",
        "    # p-value, degrees of freedom, and the expected frequencies.\n",
        "    chi2, p_value, dof, expected_frequencies = chi2_contingency(observed_data)\n",
        "    return expected_frequencies\n",
        "\n",
        "# Example Usage:\n",
        "# Observed data for a contingency table\n",
        "# Example: Gender vs. Political Party Preference\n",
        "# Rows: Male, Female\n",
        "# Columns: Republican, Democrat, Independent\n",
        "observed = np.array([[70, 50, 30],\n",
        "                     [40, 60, 50]])\n",
        "\n",
        "expected_freqs = calculate_expected_frequencies(observed)\n",
        "\n",
        "print(\"Observed Frequencies:\")\n",
        "print(observed)\n",
        "print(\"\\nExpected Frequencies:\")\n",
        "print(expected_freqs)"
      ],
      "metadata": {
        "id": "CZkk-B1QRLHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Import numpy and chi2_contingency:\n",
        "numpy is used for numerical operations, especially for handling arrays.\n",
        "chi2_contingency from scipy.stats is a powerful function that performs the Chi-square test of independence and directly provides the expected frequencies as one of its outputs.\n",
        "calculate_expected_frequencies Function:\n",
        "This function takes observed_data as input, which should be a 2D array or list of lists representing the contingency table of observed frequencies.\n",
        "It calls chi2_contingency(observed_data). This function calculates several values related to the Chi-square test, including the chi-square statistic, the p-value, the degrees of freedom, and, importantly for this request, the expected_frequencies.\n",
        "The function then returns expected_frequencies.\n",
        "Example Usage:\n",
        "An example observed contingency table is provided, representing observed frequencies for a hypothetical scenario (e.g., gender vs. political party preference).\n",
        "The calculate_expected_frequencies function is called with this observed data.\n",
        "The calculated expected_freqs are then printed, showing what frequencies would be expected under the assumption of independence between the variables."
      ],
      "metadata": {
        "id": "ARV4aQqbRQ5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution.\n",
        " - A common method to perform a goodness-of-fit test in Python is using the Chi-Square Goodness-of-Fit test, which assesses whether observed frequencies differ significantly from expected frequencies under a hypothesized distribution. The scipy.stats module provides the chisquare function for this purpose.\n",
        "Here's how to perform a Chi-Square Goodness-of-Fit test in Python:"
      ],
      "metadata": {
        "id": "q1Edhu_WRU56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Define observed frequencies\n",
        "# Example: Counts of different categories in your observed data\n",
        "observed_frequencies = np.array([25, 30, 15, 20, 10])\n",
        "\n",
        "# Define expected frequencies under the hypothesized distribution\n",
        "# These should sum to the same total as observed_frequencies\n",
        "# Example: Expected counts if the categories were equally likely\n",
        "total_observations = np.sum(observed_frequencies)\n",
        "number_of_categories = len(observed_frequencies)\n",
        "expected_frequencies = np.array([total_observations / number_of_categories] * number_of_categories)\n",
        "\n",
        "# Perform the Chi-Square Goodness-of-Fit test\n",
        "chi2_statistic, p_value = stats.chisquare(f_obs=observed_frequencies, f_exp=expected_frequencies)\n",
        "\n",
        "print(f\"Chi-Square Test Statistic: {chi2_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Significance level\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The observed data significantly differs from the expected distribution.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The observed data does not significantly differ from the expected distribution.\")"
      ],
      "metadata": {
        "id": "KmOjowYGRdFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "observed_frequencies:\n",
        "This array contains the actual counts or frequencies observed in your data for each category.\n",
        "expected_frequencies:\n",
        "This array contains the frequencies you would expect for each category if the data perfectly followed your hypothesized distribution. These must sum to the same total as the observed frequencies.\n",
        "stats.chisquare(f_obs, f_exp):\n",
        "This function calculates the Chi-Square test statistic and the corresponding p-value.\n",
        "f_obs: The array of observed frequencies.\n",
        "f_exp: The array of expected frequencies.\n",
        "Interpretation:\n",
        "Null Hypothesis (H0): The observed data fits the expected distribution.\n",
        "Alternative Hypothesis (H1): The observed data does not fit the expected distribution.\n",
        "The p-value is compared to a chosen significance level (alpha), commonly 0.05.\n",
        "If p_value < alpha, the null hypothesis is rejected, suggesting a significant difference between observed and expected distributions.\n",
        "If p_value >= alpha, the null hypothesis is not rejected, suggesting no significant difference."
      ],
      "metadata": {
        "id": "vXI3s1uCRgBT"
      }
    }
  ]
}